var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Part of the API of DynamicPPL is defined in the more lightweight interface package AbstractPPL.jl and reexported here.","category":"page"},{"location":"api/#Model","page":"API","title":"Model","text":"","category":"section"},{"location":"api/#Macros","page":"API","title":"Macros","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"A core component of DynamicPPL is the @model macro. It can be used to define probabilistic models in an intuitive way by specifying random variables and their distributions with ~ statements. These statements are rewritten by @model as calls of internal functions for sampling the variables and computing their log densities.","category":"page"},{"location":"api/","page":"API","title":"API","text":"@model","category":"page"},{"location":"api/#DynamicPPL.@model","page":"API","title":"DynamicPPL.@model","text":"@model(expr[, warn = false])\n\nMacro to specify a probabilistic model.\n\nIf warn is true, a warning is displayed if internal variable names are used in the model definition.\n\nExamples\n\nModel definition:\n\n@model function model(x, y = 42)\n    ...\nend\n\nTo generate a Model, call model(xvalue) or model(xvalue, yvalue).\n\n\n\n\n\n","category":"macro"},{"location":"api/","page":"API","title":"API","text":"One can nest models and call another model inside the model function with @submodel.","category":"page"},{"location":"api/","page":"API","title":"API","text":"@submodel","category":"page"},{"location":"api/#DynamicPPL.@submodel","page":"API","title":"DynamicPPL.@submodel","text":"@submodel model\n@submodel ... = model\n\nRun a Turing model nested inside of a Turing model.\n\nExamples\n\njulia> @model function demo1(x)\n           x ~ Normal()\n           return 1 + abs(x)\n       end;\n\njulia> @model function demo2(x, y)\n            @submodel a = demo1(x)\n            return y ~ Uniform(0, a)\n       end;\n\nWhen we sample from the model demo2(missing, 0.4) random variable x will be sampled:\n\njulia> vi = VarInfo(demo2(missing, 0.4));\n\njulia> @varname(x) in keys(vi)\ntrue\n\nVariable a is not tracked since it can be computed from the random variable x that was tracked when running demo1:\n\njulia> @varname(a) in keys(vi)\nfalse\n\nWe can check that the log joint probability of the model accumulated in vi is correct:\n\njulia> x = vi[@varname(x)];\n\njulia> getlogp(vi) ≈ logpdf(Normal(), x) + logpdf(Uniform(0, 1 + abs(x)), 0.4)\ntrue\n\n\n\n\n\n@submodel prefix=... model\n@submodel prefix=... ... = model\n\nRun a Turing model nested inside of a Turing model and add \"prefix.\" as a prefix to all random variables inside of the model.\n\nValid expressions for prefix=... are:\n\nprefix=false: no prefix is used.\nprefix=true: attempt to automatically determine the prefix from the left-hand side ... = model by first converting into a VarName, and then calling Symbol on this.\nprefix=expression: results in the prefix Symbol(expression).\n\nThe prefix makes it possible to run the same Turing model multiple times while keeping track of all random variables correctly.\n\nExamples\n\nExample models\n\njulia> @model function demo1(x)\n           x ~ Normal()\n           return 1 + abs(x)\n       end;\n\njulia> @model function demo2(x, y, z)\n            @submodel prefix=\"sub1\" a = demo1(x)\n            @submodel prefix=\"sub2\" b = demo1(y)\n            return z ~ Uniform(-a, b)\n       end;\n\nWhen we sample from the model demo2(missing, missing, 0.4) random variables sub1.x and sub2.x will be sampled:\n\njulia> vi = VarInfo(demo2(missing, missing, 0.4));\n\njulia> @varname(var\"sub1.x\") in keys(vi)\ntrue\n\njulia> @varname(var\"sub2.x\") in keys(vi)\ntrue\n\nVariables a and b are not tracked since they can be computed from the random variables sub1.x and sub2.x that were tracked when running demo1:\n\njulia> @varname(a) in keys(vi)\nfalse\n\njulia> @varname(b) in keys(vi)\nfalse\n\nWe can check that the log joint probability of the model accumulated in vi is correct:\n\njulia> sub1_x = vi[@varname(var\"sub1.x\")];\n\njulia> sub2_x = vi[@varname(var\"sub2.x\")];\n\njulia> logprior = logpdf(Normal(), sub1_x) + logpdf(Normal(), sub2_x);\n\njulia> loglikelihood = logpdf(Uniform(-1 - abs(sub1_x), 1 + abs(sub2_x)), 0.4);\n\njulia> getlogp(vi) ≈ logprior + loglikelihood\ntrue\n\nDifferent ways of setting the prefix\n\njulia> @model inner() = x ~ Normal()\ninner (generic function with 2 methods)\n\njulia> # When `prefix` is unspecified, no prefix is used.\n       @model outer() = @submodel a = inner()\nouter (generic function with 2 methods)\n\njulia> @varname(x) in keys(VarInfo(outer()))\ntrue\n\njulia> # Explicitely don't use any prefix.\n       @model outer() = @submodel prefix=false a = inner()\nouter (generic function with 2 methods)\n\njulia> @varname(x) in keys(VarInfo(outer()))\ntrue\n\njulia> # Automatically determined from `a`.\n       @model outer() = @submodel prefix=true a = inner()\nouter (generic function with 2 methods)\n\njulia> @varname(var\"a.x\") in keys(VarInfo(outer()))\ntrue\n\njulia> # Using a static string.\n       @model outer() = @submodel prefix=\"my prefix\" a = inner()\nouter (generic function with 2 methods)\n\njulia> @varname(var\"my prefix.x\") in keys(VarInfo(outer()))\ntrue\n\njulia> # Using string interpolation.\n       @model outer() = @submodel prefix=\"$(nameof(inner()))\" a = inner()\nouter (generic function with 2 methods)\n\njulia> @varname(var\"inner.x\") in keys(VarInfo(outer()))\ntrue\n\njulia> # Or using some arbitrary expression.\n       @model outer() = @submodel prefix=1 + 2 a = inner()\nouter (generic function with 2 methods)\n\njulia> @varname(var\"3.x\") in keys(VarInfo(outer()))\ntrue\n\njulia> # (×) Automatic prefixing without a left-hand side expression does not work!\n       @model outer() = @submodel prefix=true inner()\nERROR: LoadError: cannot automatically prefix with no left-hand side\n[...]\n\nNotes\n\nThe choice prefix=expression means that the prefixing will incur a runtime cost. This is also the case for prefix=true, depending on whether the expression on the the right-hand side of ... = model requires runtime-information or not, e.g. x = model will result in the static prefix x, while x[i] = model will be resolved at runtime.\n\n\n\n\n\n","category":"macro"},{"location":"api/#Type","page":"API","title":"Type","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"A Model can be created by calling the model function, as defined by @model.","category":"page"},{"location":"api/","page":"API","title":"API","text":"Model","category":"page"},{"location":"api/#DynamicPPL.Model","page":"API","title":"DynamicPPL.Model","text":"struct Model{F,argnames,defaultnames,missings,Targs,Tdefaults}\n    f::F\n    args::NamedTuple{argnames,Targs}\n    defaults::NamedTuple{defaultnames,Tdefaults}\nend\n\nA Model struct with model evaluation function of type F, arguments of names argnames types Targs, default arguments of names defaultnames with types Tdefaults, and missing arguments missings.\n\nHere argnames, defaultargnames, and missings are tuples of symbols, e.g. (:a, :b).\n\nAn argument with a type of Missing will be in missings by default. However, in non-traditional use-cases missings can be defined differently. All variables in missings are treated as random variables rather than observations.\n\nThe default arguments are used internally when constructing instances of the same model with different arguments.\n\nExamples\n\njulia> Model(f, (x = 1.0, y = 2.0))\nModel{typeof(f),(:x, :y),(),(),Tuple{Float64,Float64},Tuple{}}(f, (x = 1.0, y = 2.0), NamedTuple())\n\njulia> Model(f, (x = 1.0, y = 2.0), (x = 42,))\nModel{typeof(f),(:x, :y),(:x,),(),Tuple{Float64,Float64},Tuple{Int64}}(f, (x = 1.0, y = 2.0), (x = 42,))\n\njulia> Model{(:y,)}(f, (x = 1.0, y = 2.0), (x = 42,)) # with special definition of missings\nModel{typeof(f),(:x, :y),(:x,),(:y,),Tuple{Float64,Float64},Tuple{Int64}}(f, (x = 1.0, y = 2.0), (x = 42,))\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API","title":"API","text":"Models are callable structs.","category":"page"},{"location":"api/","page":"API","title":"API","text":"Model()","category":"page"},{"location":"api/#DynamicPPL.Model-Tuple{}","page":"API","title":"DynamicPPL.Model","text":"(model::Model)([rng, varinfo, sampler, context])\n\nSample from the model using the sampler with random number generator rng and the context, and store the sample and log joint probability in varinfo.\n\nThe method resets the log joint probability of varinfo and increases the evaluation number of sampler.\n\n\n\n\n\n","category":"method"},{"location":"api/","page":"API","title":"API","text":"Basic properties of a model can be accessed with getargnames, getmissings, and nameof.","category":"page"},{"location":"api/","page":"API","title":"API","text":"nameof(::Model)\ngetargnames\ngetmissings","category":"page"},{"location":"api/#Base.nameof-Tuple{Model}","page":"API","title":"Base.nameof","text":"nameof(model::Model)\n\nGet the name of the model as Symbol.\n\n\n\n\n\n","category":"method"},{"location":"api/#DynamicPPL.getargnames","page":"API","title":"DynamicPPL.getargnames","text":"getargnames(model::Model)\n\nGet a tuple of the argument names of the model.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.getmissings","page":"API","title":"DynamicPPL.getmissings","text":"getmissings(model::Model)\n\nGet a tuple of the names of the missing arguments of the model.\n\n\n\n\n\n","category":"function"},{"location":"api/#Evaluation","page":"API","title":"Evaluation","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"With rand one can draw samples from the prior distribution of a Model.","category":"page"},{"location":"api/","page":"API","title":"API","text":"rand","category":"page"},{"location":"api/#Base.rand","page":"API","title":"Base.rand","text":"rand([rng=Random.default_rng()], [T=NamedTuple], model::Model)\n\nGenerate a sample of type T from the prior distribution of the model.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API","title":"API","text":"One can also evaluate the log prior, log likelihood, and log joint probability.","category":"page"},{"location":"api/","page":"API","title":"API","text":"logprior\nloglikelihood\nlogjoint","category":"page"},{"location":"api/#DynamicPPL.logprior","page":"API","title":"DynamicPPL.logprior","text":"logprior(model::Model, varinfo::AbstractVarInfo)\n\nReturn the log prior probability of variables varinfo for the probabilistic model.\n\nSee also logjoint and loglikelihood.\n\n\n\n\n\nlogprior(model::Model, chain::AbstractMCMC.AbstractChains)\n\nReturn an array of log prior probabilities evaluated at each sample in an MCMC chain.\n\nExamples\n\njulia> using MCMCChains, Distributions\n\njulia> @model function demo_model(x)\n           s ~ InverseGamma(2, 3)\n           m ~ Normal(0, sqrt(s))\n           for i in eachindex(x)\n               x[i] ~ Normal(m, sqrt(s))\n           end\n       end;\n\njulia> # construct a chain of samples using MCMCChains\n       chain = Chains(rand(10, 2, 3), [:s, :m]);\n\njulia> logprior(demo_model([1., 2.]), chain);\n\n\n\n\n\nlogprior(model::Model, θ)\n\nReturn the log prior probability of variables θ for the probabilistic model.\n\nSee also logjoint and loglikelihood.\n\nExamples\n\njulia> @model function demo(x)\n           m ~ Normal()\n           for i in eachindex(x)\n               x[i] ~ Normal(m, 1.0)\n           end\n       end\ndemo (generic function with 2 methods)\n\njulia> # Using a `NamedTuple`.\n       logprior(demo([1.0]), (m = 100.0, ))\n-5000.918938533205\n\njulia> # Using a `OrderedDict`.\n       logprior(demo([1.0]), OrderedDict(@varname(m) => 100.0))\n-5000.918938533205\n\njulia> # Truth.\n       logpdf(Normal(), 100.0)\n-5000.918938533205\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsAPI.loglikelihood","page":"API","title":"StatsAPI.loglikelihood","text":"loglikelihood(model::Model, varinfo::AbstractVarInfo)\n\nReturn the log likelihood of variables varinfo for the probabilistic model.\n\nSee also logjoint and logprior.\n\n\n\n\n\nloglikelihood(model::Model, chain::AbstractMCMC.AbstractChains)\n\nReturn an array of log likelihoods evaluated at each sample in an MCMC chain.\n\nExamples\n\njulia> using MCMCChains, Distributions\n\njulia> @model function demo_model(x)\n           s ~ InverseGamma(2, 3)\n           m ~ Normal(0, sqrt(s))\n           for i in eachindex(x)\n               x[i] ~ Normal(m, sqrt(s))\n           end\n       end;\n\njulia> # construct a chain of samples using MCMCChains\n       chain = Chains(rand(10, 2, 3), [:s, :m]);\n\njulia> loglikelihood(demo_model([1., 2.]), chain);\n\n\n\n\n\nloglikelihood(model::Model, θ)\n\nReturn the log likelihood of variables θ for the probabilistic model.\n\nSee also logjoint and logprior.\n\nExamples\n\njulia> @model function demo(x)\n           m ~ Normal()\n           for i in eachindex(x)\n               x[i] ~ Normal(m, 1.0)\n           end\n       end\ndemo (generic function with 2 methods)\n\njulia> # Using a `NamedTuple`.\n       loglikelihood(demo([1.0]), (m = 100.0, ))\n-4901.418938533205\n\njulia> # Using a `OrderedDict`.\n       loglikelihood(demo([1.0]), OrderedDict(@varname(m) => 100.0))\n-4901.418938533205\n\njulia> # Truth.\n       logpdf(Normal(100.0, 1.0), 1.0)\n-4901.418938533205\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.logjoint","page":"API","title":"DynamicPPL.logjoint","text":"logjoint(model::Model, varinfo::AbstractVarInfo)\n\nReturn the log joint probability of variables varinfo for the probabilistic model.\n\nSee logjoint and loglikelihood.\n\n\n\n\n\nlogjoint(model::Model, chain::AbstractMCMC.AbstractChains)\n\nReturn an array of log joint probabilities evaluated at each sample in an MCMC chain.\n\nExamples\n\njulia> using MCMCChains, Distributions\n\njulia> @model function demo_model(x)\n           s ~ InverseGamma(2, 3)\n           m ~ Normal(0, sqrt(s))\n           for i in eachindex(x)\n               x[i] ~ Normal(m, sqrt(s))\n           end\n       end;\n\njulia> # construct a chain of samples using MCMCChains\n       chain = Chains(rand(10, 2, 3), [:s, :m]);\n\njulia> logjoint(demo_model([1., 2.]), chain);\n\n\n\n\n\nlogjoint(model::Model, θ)\n\nReturn the log joint probability of variables θ for the probabilistic model.\n\nSee logjoint and loglikelihood.\n\nExamples\n\njulia> @model function demo(x)\n           m ~ Normal()\n           for i in eachindex(x)\n               x[i] ~ Normal(m, 1.0)\n           end\n       end\ndemo (generic function with 2 methods)\n\njulia> # Using a `NamedTuple`.\n       logjoint(demo([1.0]), (m = 100.0, ))\n-9902.33787706641\n\njulia> # Using a `OrderedDict`.\n       logjoint(demo([1.0]), OrderedDict(@varname(m) => 100.0))\n-9902.33787706641\n\njulia> # Truth.\n       logpdf(Normal(100.0, 1.0), 1.0) + logpdf(Normal(), 100.0)\n-9902.33787706641\n\n\n\n\n\n","category":"function"},{"location":"api/#LogDensityProblems.jl-interface","page":"API","title":"LogDensityProblems.jl interface","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"The LogDensityProblems.jl interface is also supported by simply wrapping a Model in a DynamicPPL.LogDensityFunction:","category":"page"},{"location":"api/","page":"API","title":"API","text":"DynamicPPL.LogDensityFunction","category":"page"},{"location":"api/#DynamicPPL.LogDensityFunction","page":"API","title":"DynamicPPL.LogDensityFunction","text":"LogDensityFunction\n\nA callable representing a log density function of a model.\n\nFields\n\nvarinfo: varinfo used for evaluation\nmodel: model used for evaluation\ncontext: context used for evaluation\n\nExamples\n\njulia> using Distributions\n\njulia> using DynamicPPL: LogDensityFunction, contextualize\n\njulia> @model function demo(x)\n           m ~ Normal()\n           x ~ Normal(m, 1)\n       end\ndemo (generic function with 2 methods)\n\njulia> model = demo(1.0);\n\njulia> f = LogDensityFunction(model);\n\njulia> # It implements the interface of LogDensityProblems.jl.\n       using LogDensityProblems\n\njulia> LogDensityProblems.logdensity(f, [0.0])\n-2.3378770664093453\n\njulia> LogDensityProblems.dimension(f)\n1\n\njulia> # By default it uses `VarInfo` under the hood, but this is not necessary.\n       f = LogDensityFunction(model, SimpleVarInfo(model));\n\njulia> LogDensityProblems.logdensity(f, [0.0])\n-2.3378770664093453\n\njulia> # This also respects the context in `model`.\n       f_prior = LogDensityFunction(contextualize(model, DynamicPPL.PriorContext()), VarInfo(model));\n\njulia> LogDensityProblems.logdensity(f_prior, [0.0]) == logpdf(Normal(), 0.0)\ntrue\n\n\n\n\n\n","category":"type"},{"location":"api/#Condition-and-decondition","page":"API","title":"Condition and decondition","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"A Model can be conditioned on a set of observations with AbstractPPL.condition or its alias |.","category":"page"},{"location":"api/","page":"API","title":"API","text":"|(::Model, ::Any)\ncondition\nDynamicPPL.conditioned","category":"page"},{"location":"api/#Base.:|-Tuple{Model, Any}","page":"API","title":"Base.:|","text":"model | (x = 1.0, ...)\n\nReturn a Model which now treats variables on the right-hand side as observations.\n\nSee condition for more information and examples.\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractPPL.condition","page":"API","title":"AbstractPPL.condition","text":"condition(model::Model; values...)\ncondition(model::Model, values::NamedTuple)\n\nReturn a Model which now treats the variables in values as observations.\n\nSee also: decondition, conditioned\n\nLimitations\n\nThis does currently not work with variables that are provided to the model as arguments, e.g. @model function demo(x) ... end means that condition will not affect the variable x.\n\nTherefore if one wants to make use of condition and decondition one should not be specifying any random variables as arguments.\n\nThis is done for the sake of backwards compatibility.\n\nExamples\n\nSimple univariate model\n\njulia> using Distributions\n\njulia> @model function demo()\n           m ~ Normal()\n           x ~ Normal(m, 1)\n           return (; m=m, x=x)\n       end\ndemo (generic function with 2 methods)\n\njulia> model = demo();\n\njulia> m, x = model(); (m ≠ 1.0 && x ≠ 100.0)\ntrue\n\njulia> # Create a new instance which treats `x` as observed\n       # with value `100.0`, and similarly for `m=1.0`.\n       conditioned_model = condition(model, x=100.0, m=1.0);\n\njulia> m, x = conditioned_model(); (m == 1.0 && x == 100.0)\ntrue\n\njulia> # Let's only condition on `x = 100.0`.\n       conditioned_model = condition(model, x = 100.0);\n\njulia> m, x =conditioned_model(); (m ≠ 1.0 && x == 100.0)\ntrue\n\njulia> # We can also use the nicer `|` syntax.\n       conditioned_model = model | (x = 100.0, );\n\njulia> m, x = conditioned_model(); (m ≠ 1.0 && x == 100.0)\ntrue\n\nThe above uses a NamedTuple to hold the conditioning variables, which allows us to perform some additional optimizations; in many cases, the above has zero runtime-overhead.\n\nBut we can also use a Dict, which offers more flexibility in the conditioning (see examples further below) but generally has worse performance than the NamedTuple approach:\n\njulia> conditioned_model_dict = condition(model, Dict(@varname(x) => 100.0));\n\njulia> m, x = conditioned_model_dict(); (m ≠ 1.0 && x == 100.0)\ntrue\n\njulia> # There's also an option using `|` by letting the right-hand side be a tuple\n       # with elements of type `Pair{<:VarName}`, i.e. `vn => value` with `vn isa VarName`.\n       conditioned_model_dict = model | (@varname(x) => 100.0, );\n\njulia> m, x = conditioned_model_dict(); (m ≠ 1.0 && x == 100.0)\ntrue\n\nCondition only a part of a multivariate variable\n\nNot only can be condition on multivariate random variables, but we can also use the standard mechanism of setting something to missing in the call to condition to only condition on a part of the variable.\n\njulia> @model function demo_mv(::Type{TV}=Float64) where {TV}\n           m = Vector{TV}(undef, 2)\n           m[1] ~ Normal()\n           m[2] ~ Normal()\n           return m\n       end\ndemo_mv (generic function with 4 methods)\n\njulia> model = demo_mv();\n\njulia> conditioned_model = condition(model, m = [missing, 1.0]);\n\njulia> # (✓) `m[1]` sampled while `m[2]` is fixed\n       m = conditioned_model(); (m[1] ≠ 1.0 && m[2] == 1.0)\ntrue\n\nIntuitively one might also expect to be able to write model | (m[1] = 1.0, ). Unfortunately this is not supported as it has the potential of increasing compilation times but without offering any benefit with respect to runtime:\n\njulia> # (×) `m[2]` is not set to 1.0.\n       m = condition(model, var\"m[2]\" = 1.0)(); m[2] == 1.0\nfalse\n\nBut you can do this if you use a Dict as the underlying storage instead:\n\njulia> # Alternatives:\n       # - `model | (@varname(m[2]) => 1.0,)`\n       # - `condition(model, Dict(@varname(m[2] => 1.0)))`\n       # (✓) `m[2]` is set to 1.0.\n       m = condition(model, @varname(m[2]) => 1.0)(); (m[1] ≠ 1.0 && m[2] == 1.0)\ntrue\n\nNested models\n\ncondition of course also supports the use of nested models through the use of @submodel.\n\njulia> @model demo_inner() = m ~ Normal()\ndemo_inner (generic function with 2 methods)\n\njulia> @model function demo_outer()\n           @submodel m = demo_inner()\n           return m\n       end\ndemo_outer (generic function with 2 methods)\n\njulia> model = demo_outer();\n\njulia> model() ≠ 1.0\ntrue\n\njulia> conditioned_model = model | (m = 1.0, );\n\njulia> conditioned_model()\n1.0\n\nBut one needs to be careful when prefixing variables in the nested models:\n\njulia> @model function demo_outer_prefix()\n           @submodel prefix=\"inner\" m = demo_inner()\n           return m\n       end\ndemo_outer_prefix (generic function with 2 methods)\n\njulia> # (×) This doesn't work now!\n       conditioned_model = demo_outer_prefix() | (m = 1.0, );\n\njulia> conditioned_model() == 1.0\nfalse\n\njulia> # (✓) `m` in `demo_inner` is referred to as `inner.m` internally, so we do:\n       conditioned_model = demo_outer_prefix() | (var\"inner.m\" = 1.0, );\n\njulia> conditioned_model()\n1.0\n\njulia> # Note that the above `var\"...\"` is just standard Julia syntax:\n       keys((var\"inner.m\" = 1.0, ))\n(Symbol(\"inner.m\"),)\n\nAnd similarly when using Dict:\n\njulia> conditioned_model_dict = demo_outer_prefix() | (@varname(var\"inner.m\") => 1.0);\n\njulia> conditioned_model_dict()\n1.0\n\nThe difference is maybe more obvious once we look at how these different in their trace/VarInfo:\n\njulia> keys(VarInfo(demo_outer()))\n1-element Vector{VarName{:m, Setfield.IdentityLens}}:\n m\n\njulia> keys(VarInfo(demo_outer_prefix()))\n1-element Vector{VarName{Symbol(\"inner.m\"), Setfield.IdentityLens}}:\n inner.m\n\nFrom this we can tell what the correct way to condition m within demo_inner is in the two different models.\n\n\n\n\n\ncondition([context::AbstractContext,] values::NamedTuple)\ncondition([context::AbstractContext]; values...)\n\nReturn ConditionContext with values and context if values is non-empty, otherwise return context which is DefaultContext by default.\n\nSee also: decondition\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.conditioned","page":"API","title":"DynamicPPL.conditioned","text":"conditioned(model::Model)\n\nReturn the conditioned values in model.\n\nExamples\n\njulia> using Distributions\n\njulia> using DynamicPPL: conditioned, contextualize\n\njulia> @model function demo()\n           m ~ Normal()\n           x ~ Normal(m, 1)\n       end\ndemo (generic function with 2 methods)\n\njulia> m = demo();\n\njulia> # Returns all the variables we have conditioned on + their values.\n       conditioned(condition(m, x=100.0, m=1.0))\n(x = 100.0, m = 1.0)\n\njulia> # Nested ones also work (note that `PrefixContext` does nothing to the result).\n       cm = condition(contextualize(m, PrefixContext{:a}(condition(m=1.0))), x=100.0);\n\njulia> conditioned(cm)\n(x = 100.0, m = 1.0)\n\njulia> # Since we conditioned on `m`, not `a.m` as it will appear after prefixed,\n       # `a.m` is treated as a random variable.\n       keys(VarInfo(cm))\n1-element Vector{VarName{Symbol(\"a.m\"), Setfield.IdentityLens}}:\n a.m\n\njulia> # If we instead condition on `a.m`, `m` in the model will be considered an observation.\n       cm = condition(contextualize(m, PrefixContext{:a}(condition(var\"a.m\"=1.0))), x=100.0);\n\njulia> conditioned(cm).x\n100.0\n\njulia> conditioned(cm).var\"a.m\"\n1.0\n\njulia> keys(VarInfo(cm)) # <= no variables are sampled\nVarName[]\n\n\n\n\n\nconditioned(context::AbstractContext)\n\nReturn NamedTuple of values that are conditioned on under context`.\n\nNote that this will recursively traverse the context stack and return a merged version of the condition values.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API","title":"API","text":"Similarly, one can specify with AbstractPPL.decondition that certain, or all, random variables are not observed.","category":"page"},{"location":"api/","page":"API","title":"API","text":"decondition","category":"page"},{"location":"api/#AbstractPPL.decondition","page":"API","title":"AbstractPPL.decondition","text":"decondition(model::Model)\ndecondition(model::Model, variables...)\n\nReturn a Model for which variables... are not considered observations. If no variables are provided, then all variables currently considered observations will no longer be.\n\nThis is essentially the inverse of condition. This also means that it suffers from the same limitiations.\n\nNote that currently we only support variables to take on explicit values provided to condition.\n\nExamples\n\njulia> using Distributions\n\njulia> @model function demo()\n           m ~ Normal()\n           x ~ Normal(m, 1)\n           return (; m=m, x=x)\n       end\ndemo (generic function with 2 methods)\n\njulia> conditioned_model = condition(demo(), m = 1.0, x = 10.0);\n\njulia> conditioned_model()\n(m = 1.0, x = 10.0)\n\njulia> # By specifying the `VarName` to `decondition`.\n       model = decondition(conditioned_model, @varname(m));\n\njulia> (m, x) = model(); (m ≠ 1.0 && x == 10.0)\ntrue\n\njulia> # When `NamedTuple` is used as the underlying, you can also provide\n       # the symbol directly (though the `@varname` approach is preferable if\n       # if the variable is known at compile-time).\n       model = decondition(conditioned_model, :m);\n\njulia> (m, x) = model(); (m ≠ 1.0 && x == 10.0)\ntrue\n\njulia> # `decondition` multiple at once:\n       (m, x) = decondition(model, :m, :x)(); (m ≠ 1.0 && x ≠ 10.0)\ntrue\n\njulia> # `decondition` without any symbols will `decondition` all variables.\n       (m, x) = decondition(model)(); (m ≠ 1.0 && x ≠ 10.0)\ntrue\n\njulia> # Usage of `Val` to perform `decondition` at compile-time if possible\n       # is also supported.\n       model = decondition(conditioned_model, Val{:m}());\n\njulia> (m, x) = model(); (m ≠ 1.0 && x == 10.0)\ntrue\n\nSimilarly when using a Dict:\n\njulia> conditioned_model_dict = condition(demo(), @varname(m) => 1.0, @varname(x) => 10.0);\n\njulia> conditioned_model_dict()\n(m = 1.0, x = 10.0)\n\njulia> deconditioned_model_dict = decondition(conditioned_model_dict, @varname(m));\n\njulia> (m, x) = deconditioned_model_dict(); m ≠ 1.0 && x == 10.0\ntrue\n\nBut, as mentioned, decondition is only supported for variables explicitly provided to condition earlier;\n\njulia> @model function demo_mv(::Type{TV}=Float64) where {TV}\n           m = Vector{TV}(undef, 2)\n           m[1] ~ Normal()\n           m[2] ~ Normal()\n           return m\n       end\ndemo_mv (generic function with 4 methods)\n\njulia> model = demo_mv();\n\njulia> conditioned_model = condition(model, @varname(m) => [1.0, 2.0]);\n\njulia> conditioned_model()\n2-element Vector{Float64}:\n 1.0\n 2.0\n\njulia> deconditioned_model = decondition(conditioned_model, @varname(m[1]));\n\njulia> deconditioned_model()  # (×) `m[1]` is still conditioned\n2-element Vector{Float64}:\n 1.0\n 2.0\n\njulia> # (✓) this works though\n       deconditioned_model_2 = deconditioned_model | (@varname(m[1]) => missing);\n\njulia> m = deconditioned_model_2(); (m[1] ≠ 1.0 && m[2] == 2.0)\ntrue\n\n\n\n\n\ndecondition(context::AbstractContext, syms...)\n\nReturn context but with syms no longer conditioned on.\n\nNote that this recursively traverses contexts, deconditioning all along the way.\n\nSee also: condition\n\n\n\n\n\n","category":"function"},{"location":"api/#Fixing-and-unfixing","page":"API","title":"Fixing and unfixing","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"We can also fix a collection of variables in a Model to certain using fix.","category":"page"},{"location":"api/","page":"API","title":"API","text":"This might seem quite similar to the aforementioned condition and its siblings, but they are indeed different operations:","category":"page"},{"location":"api/","page":"API","title":"API","text":"conditioned variables are considered to be observations, and are thus included in the computation logjoint and loglikelihood, but not in logprior.\nfixed variables are considered to be constant, and are thus not included in any log-probability computations.","category":"page"},{"location":"api/","page":"API","title":"API","text":"The differences are more clearly spelled out in the docstring of fix below.","category":"page"},{"location":"api/","page":"API","title":"API","text":"fix\nDynamicPPL.fixed","category":"page"},{"location":"api/#DynamicPPL.fix","page":"API","title":"DynamicPPL.fix","text":"fix(model::Model; values...)\nfix(model::Model, values::NamedTuple)\n\nReturn a Model which now treats the variables in values as fixed.\n\nSee also: unfix, fixed\n\nExamples\n\nSimple univariate model\n\njulia> using Distributions\n\njulia> @model function demo()\n           m ~ Normal()\n           x ~ Normal(m, 1)\n           return (; m=m, x=x)\n       end\ndemo (generic function with 2 methods)\n\njulia> model = demo();\n\njulia> m, x = model(); (m ≠ 1.0 && x ≠ 100.0)\ntrue\n\njulia> # Create a new instance which treats `x` as observed\n       # with value `100.0`, and similarly for `m=1.0`.\n       fixed_model = fix(model, x=100.0, m=1.0);\n\njulia> m, x = fixed_model(); (m == 1.0 && x == 100.0)\ntrue\n\njulia> # Let's only fix on `x = 100.0`.\n       fixed_model = fix(model, x = 100.0);\n\njulia> m, x = fixed_model(); (m ≠ 1.0 && x == 100.0)\ntrue\n\nThe above uses a NamedTuple to hold the fixed variables, which allows us to perform some additional optimizations; in many cases, the above has zero runtime-overhead.\n\nBut we can also use a Dict, which offers more flexibility in the fixing (see examples further below) but generally has worse performance than the NamedTuple approach:\n\njulia> fixed_model_dict = fix(model, Dict(@varname(x) => 100.0));\n\njulia> m, x = fixed_model_dict(); (m ≠ 1.0 && x == 100.0)\ntrue\n\njulia> # Alternative: pass `Pair{<:VarName}` as positional argument.\n       fixed_model_dict = fix(model, @varname(x) => 100.0, );\n\njulia> m, x = fixed_model_dict(); (m ≠ 1.0 && x == 100.0)\ntrue\n\nFix only a part of a multivariate variable\n\nWe can not only fix multivariate random variables, but we can also use the standard mechanism of setting something to missing in the call to fix to only fix a part of the variable.\n\njulia> @model function demo_mv(::Type{TV}=Float64) where {TV}\n           m = Vector{TV}(undef, 2)\n           m[1] ~ Normal()\n           m[2] ~ Normal()\n           return m\n       end\ndemo_mv (generic function with 4 methods)\n\njulia> model = demo_mv();\n\njulia> fixed_model = fix(model, m = [missing, 1.0]);\n\njulia> # (✓) `m[1]` sampled while `m[2]` is fixed\n       m = fixed_model(); (m[1] ≠ 1.0 && m[2] == 1.0)\ntrue\n\nIntuitively one might also expect to be able to write something like fix(model, var\"m[1]\" = 1.0, ). Unfortunately this is not supported as it has the potential of increasing compilation times but without offering any benefit with respect to runtime:\n\njulia> # (×) `m[2]` is not set to 1.0.\n       m = fix(model, var\"m[2]\" = 1.0)(); m[2] == 1.0\nfalse\n\nBut you can do this if you use a Dict as the underlying storage instead:\n\njulia> # Alternative: `fix(model, Dict(@varname(m[2] => 1.0)))`\n       # (✓) `m[2]` is set to 1.0.\n       m = fix(model, @varname(m[2]) => 1.0)(); (m[1] ≠ 1.0 && m[2] == 1.0)\ntrue\n\nNested models\n\nfix of course also supports the use of nested models through the use of @submodel.\n\njulia> @model demo_inner() = m ~ Normal()\ndemo_inner (generic function with 2 methods)\n\njulia> @model function demo_outer()\n           @submodel m = demo_inner()\n           return m\n       end\ndemo_outer (generic function with 2 methods)\n\njulia> model = demo_outer();\n\njulia> model() ≠ 1.0\ntrue\n\njulia> fixed_model = model | (m = 1.0, );\n\njulia> fixed_model()\n1.0\n\nBut one needs to be careful when prefixing variables in the nested models:\n\njulia> @model function demo_outer_prefix()\n           @submodel prefix=\"inner\" m = demo_inner()\n           return m\n       end\ndemo_outer_prefix (generic function with 2 methods)\n\njulia> # (×) This doesn't work now!\n       fixed_model = demo_outer_prefix() | (m = 1.0, );\n\njulia> fixed_model() == 1.0\nfalse\n\njulia> # (✓) `m` in `demo_inner` is referred to as `inner.m` internally, so we do:\n       fixed_model = demo_outer_prefix() | (var\"inner.m\" = 1.0, );\n\njulia> fixed_model()\n1.0\n\njulia> # Note that the above `var\"...\"` is just standard Julia syntax:\n       keys((var\"inner.m\" = 1.0, ))\n(Symbol(\"inner.m\"),)\n\nAnd similarly when using Dict:\n\njulia> fixed_model_dict = demo_outer_prefix() | (@varname(var\"inner.m\") => 1.0);\n\njulia> fixed_model_dict()\n1.0\n\nThe difference is maybe more obvious once we look at how these different in their trace/VarInfo:\n\njulia> keys(VarInfo(demo_outer()))\n1-element Vector{VarName{:m, Setfield.IdentityLens}}:\n m\n\njulia> keys(VarInfo(demo_outer_prefix()))\n1-element Vector{VarName{Symbol(\"inner.m\"), Setfield.IdentityLens}}:\n inner.m\n\nFrom this we can tell what the correct way to fix m within demo_inner is in the two different models.\n\nDifference from condition\n\nA very similar functionality is also provided by condition which, not surprisingly, conditions variables instead of fixing them. The only difference between fixing and conditioning is as follows:\n\nconditioned variables are considered to be observations, and are thus included in the computation logjoint and loglikelihood, but not in logprior.\nfixed variables are considered to be constant, and are thus not included in any log-probability computations.\n\njulia> @model function demo()\n           m ~ Normal()\n           x ~ Normal(m, 1)\n           return (; m=m, x=x)\n       end\ndemo (generic function with 2 methods)\n\njulia> model = demo();\n\njulia> model_fixed = fix(model, m = 1.0);\n\njulia> model_conditioned = condition(model, m = 1.0);\n\njulia> logjoint(model_fixed, (x=1.0,))\n-0.9189385332046728\n\njulia> # Different!\n       logjoint(model_conditioned, (x=1.0,))\n-2.3378770664093453\n\njulia> # And the difference is the missing log-probability of `m`:\n       logjoint(model_fixed, (x=1.0,)) + logpdf(Normal(), 1.0) == logjoint(model_conditioned, (x=1.0,))\ntrue\n\n\n\n\n\nfix([context::AbstractContext,] values::NamedTuple)\nfix([context::AbstractContext]; values...)\n\nReturn FixedContext with values and context if values is non-empty, otherwise return context which is DefaultContext by default.\n\nSee also: unfix\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.fixed","page":"API","title":"DynamicPPL.fixed","text":"fixed(model::Model)\n\nReturn the fixed values in model.\n\nExamples\n\njulia> using Distributions\n\njulia> using DynamicPPL: fixed, contextualize\n\njulia> @model function demo()\n           m ~ Normal()\n           x ~ Normal(m, 1)\n       end\ndemo (generic function with 2 methods)\n\njulia> m = demo();\n\njulia> # Returns all the variables we have fixed on + their values.\n       fixed(fix(m, x=100.0, m=1.0))\n(x = 100.0, m = 1.0)\n\njulia> # Nested ones also work (note that `PrefixContext` does nothing to the result).\n       cm = fix(contextualize(m, PrefixContext{:a}(fix(m=1.0))), x=100.0);\n\njulia> fixed(cm)\n(x = 100.0, m = 1.0)\n\njulia> # Since we fixed on `m`, not `a.m` as it will appear after prefixed,\n       # `a.m` is treated as a random variable.\n       keys(VarInfo(cm))\n1-element Vector{VarName{Symbol(\"a.m\"), Setfield.IdentityLens}}:\n a.m\n\njulia> # If we instead fix on `a.m`, `m` in the model will be considered an observation.\n       cm = fix(contextualize(m, PrefixContext{:a}(fix(var\"a.m\"=1.0))), x=100.0);\n\njulia> fixed(cm).x\n100.0\n\njulia> fixed(cm).var\"a.m\"\n1.0\n\njulia> keys(VarInfo(cm)) # <= no variables are sampled\nVarName[]\n\n\n\n\n\nfixed(context::AbstractContext)\n\nReturn the values that are fixed under context.\n\nNote that this will recursively traverse the context stack and return a merged version of the fix values.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API","title":"API","text":"The difference between fix and condition is described in the docstring of fix above.","category":"page"},{"location":"api/","page":"API","title":"API","text":"Similarly, we can unfix variables, i.e. return them to their original meaning:","category":"page"},{"location":"api/","page":"API","title":"API","text":"unfix","category":"page"},{"location":"api/#DynamicPPL.unfix","page":"API","title":"DynamicPPL.unfix","text":"unfix(model::Model)\nunfix(model::Model, variables...)\n\nReturn a Model for which variables... are not considered fixed. If no variables are provided, then all variables currently considered fixed will no longer be.\n\nThis is essentially the inverse of fix. This also means that it suffers from the same limitiations.\n\nNote that currently we only support variables to take on explicit values provided to fix.\n\nExamples\n\njulia> using Distributions\n\njulia> @model function demo()\n           m ~ Normal()\n           x ~ Normal(m, 1)\n           return (; m=m, x=x)\n       end\ndemo (generic function with 2 methods)\n\njulia> fixed_model = fix(demo(), m = 1.0, x = 10.0);\n\njulia> fixed_model()\n(m = 1.0, x = 10.0)\n\njulia> # By specifying the `VarName` to `unfix`.\n       model = unfix(fixed_model, @varname(m));\n\njulia> (m, x) = model(); (m ≠ 1.0 && x == 10.0)\ntrue\n\njulia> # When `NamedTuple` is used as the underlying, you can also provide\n       # the symbol directly (though the `@varname` approach is preferable if\n       # if the variable is known at compile-time).\n       model = unfix(fixed_model, :m);\n\njulia> (m, x) = model(); (m ≠ 1.0 && x == 10.0)\ntrue\n\njulia> # `unfix` multiple at once:\n       (m, x) = unfix(model, :m, :x)(); (m ≠ 1.0 && x ≠ 10.0)\ntrue\n\njulia> # `unfix` without any symbols will `unfix` all variables.\n       (m, x) = unfix(model)(); (m ≠ 1.0 && x ≠ 10.0)\ntrue\n\njulia> # Usage of `Val` to perform `unfix` at compile-time if possible\n       # is also supported.\n       model = unfix(fixed_model, Val{:m}());\n\njulia> (m, x) = model(); (m ≠ 1.0 && x == 10.0)\ntrue\n\nSimilarly when using a Dict:\n\njulia> fixed_model_dict = fix(demo(), @varname(m) => 1.0, @varname(x) => 10.0);\n\njulia> fixed_model_dict()\n(m = 1.0, x = 10.0)\n\njulia> unfixed_model_dict = unfix(fixed_model_dict, @varname(m));\n\njulia> (m, x) = unfixed_model_dict(); m ≠ 1.0 && x == 10.0\ntrue\n\nBut, as mentioned, unfix is only supported for variables explicitly provided to fix earlier:\n\njulia> @model function demo_mv(::Type{TV}=Float64) where {TV}\n           m = Vector{TV}(undef, 2)\n           m[1] ~ Normal()\n           m[2] ~ Normal()\n           return m\n       end\ndemo_mv (generic function with 4 methods)\n\njulia> model = demo_mv();\n\njulia> fixed_model = fix(model, @varname(m) => [1.0, 2.0]);\n\njulia> fixed_model()\n2-element Vector{Float64}:\n 1.0\n 2.0\n\njulia> unfixed_model = unfix(fixed_model, @varname(m[1]));\n\njulia> unfixed_model()  # (×) `m[1]` is still fixed\n2-element Vector{Float64}:\n 1.0\n 2.0\n\njulia> # (✓) this works though\n       unfixed_model_2 = fix(unfixed_model, @varname(m[1]) => missing);\n\njulia> m = unfixed_model_2(); (m[1] ≠ 1.0 && m[2] == 2.0)\ntrue\n\n\n\n\n\nunfix(context::AbstractContext, syms...)\n\nReturn context but with syms no longer fixed.\n\nNote that this recursively traverses contexts, unfixing all along the way.\n\nSee also: fix\n\n\n\n\n\n","category":"function"},{"location":"api/#Utilities","page":"API","title":"Utilities","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"It is possible to manually increase (or decrease) the accumulated log density from within a model function.","category":"page"},{"location":"api/","page":"API","title":"API","text":"@addlogprob!","category":"page"},{"location":"api/#DynamicPPL.@addlogprob!","page":"API","title":"DynamicPPL.@addlogprob!","text":"@addlogprob!(ex)\n\nAdd the result of the evaluation of ex to the joint log probability.\n\nExamples\n\nThis macro allows you to include arbitrary terms in the likelihood\n\njulia> myloglikelihood(x, μ) = loglikelihood(Normal(μ, 1), x);\n\njulia> @model function demo(x)\n           μ ~ Normal()\n           @addlogprob! myloglikelihood(x, μ)\n       end;\n\njulia> x = [1.3, -2.1];\n\njulia> loglikelihood(demo(x), (μ=0.2,)) ≈ myloglikelihood(x, 0.2)\ntrue\n\nand to reject samples:\n\njulia> @model function demo(x)\n           m ~ MvNormal(zero(x), I)\n           if dot(m, x) < 0\n               @addlogprob! -Inf\n               # Exit the model evaluation early\n               return\n           end\n           x ~ MvNormal(m, I)\n           return\n       end;\n\njulia> logjoint(demo([-2.1]), (m=[0.2],)) == -Inf\ntrue\n\nnote: Note\nThe @addlogprob! macro increases the accumulated log probability regardless of the evaluation context, i.e., regardless of whether you evaluate the log prior, the log likelihood or the log joint density. If you would like to avoid this behaviour you should check the evaluation context. It can be accessed with the internal variable __context__. For instance, in the following example the log density is not accumulated when only the log prior is computed:  julia> myloglikelihood(x, μ) = loglikelihood(Normal(μ, 1), x);\n\njulia> @model function demo(x)\n           μ ~ Normal()\n           if DynamicPPL.leafcontext(__context__) !== PriorContext()\n               @addlogprob! myloglikelihood(x, μ)\n           end\n       end;\n\njulia> x = [1.3, -2.1];\n\njulia> logprior(demo(x), (μ=0.2,)) ≈ logpdf(Normal(), 0.2)\ntrue\n\njulia> loglikelihood(demo(x), (μ=0.2,)) ≈ myloglikelihood(x, 0.2)\ntrue\n\n\n\n\n\n","category":"macro"},{"location":"api/","page":"API","title":"API","text":"Return values of the model function for a collection of samples can be obtained with generated_quantities.","category":"page"},{"location":"api/","page":"API","title":"API","text":"generated_quantities","category":"page"},{"location":"api/#DynamicPPL.generated_quantities","page":"API","title":"DynamicPPL.generated_quantities","text":"generated_quantities(model::Model, chain::AbstractChains)\n\nExecute model for each of the samples in chain and return an array of the values returned by the model for each sample.\n\nExamples\n\nGeneral\n\nOften you might have additional quantities computed inside the model that you want to inspect, e.g.\n\n@model function demo(x)\n    # sample and observe\n    θ ~ Prior()\n    x ~ Likelihood()\n    return interesting_quantity(θ, x)\nend\nm = demo(data)\nchain = sample(m, alg, n)\n# To inspect the `interesting_quantity(θ, x)` where `θ` is replaced by samples\n# from the posterior/`chain`:\ngenerated_quantities(m, chain) # <= results in a `Vector` of returned values\n                               #    from `interesting_quantity(θ, x)`\n\nConcrete (and simple)\n\njulia> using DynamicPPL, Turing\n\njulia> @model function demo(xs)\n           s ~ InverseGamma(2, 3)\n           m_shifted ~ Normal(10, √s)\n           m = m_shifted - 10\n\n           for i in eachindex(xs)\n               xs[i] ~ Normal(m, √s)\n           end\n\n           return (m, )\n       end\ndemo (generic function with 1 method)\n\njulia> model = demo(randn(10));\n\njulia> chain = sample(model, MH(), 10);\n\njulia> generated_quantities(model, chain)\n10×1 Array{Tuple{Float64},2}:\n (2.1964758025119338,)\n (2.1964758025119338,)\n (0.09270081916291417,)\n (0.09270081916291417,)\n (0.09270081916291417,)\n (0.09270081916291417,)\n (0.09270081916291417,)\n (0.043088571494005024,)\n (-0.16489786710222099,)\n (-0.16489786710222099,)\n\n\n\n\n\ngenerated_quantities(model::Model, parameters::NamedTuple)\ngenerated_quantities(model::Model, values, keys)\ngenerated_quantities(model::Model, values, keys)\n\nExecute model with variables keys set to values and return the values returned by the model.\n\nIf a NamedTuple is given, keys=keys(parameters) and values=values(parameters).\n\nExample\n\njulia> using DynamicPPL, Distributions\n\njulia> @model function demo(xs)\n           s ~ InverseGamma(2, 3)\n           m_shifted ~ Normal(10, √s)\n           m = m_shifted - 10\n           for i in eachindex(xs)\n               xs[i] ~ Normal(m, √s)\n           end\n           return (m, )\n       end\ndemo (generic function with 2 methods)\n\njulia> model = demo(randn(10));\n\njulia> parameters = (; s = 1.0, m_shifted=10);\n\njulia> generated_quantities(model, parameters)\n(0.0,)\n\njulia> generated_quantities(model, values(parameters), keys(parameters))\n(0.0,)\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API","title":"API","text":"For a chain of samples, one can compute the pointwise log-likelihoods of each observed random variable with pointwise_loglikelihoods.","category":"page"},{"location":"api/","page":"API","title":"API","text":"pointwise_loglikelihoods","category":"page"},{"location":"api/#DynamicPPL.pointwise_loglikelihoods","page":"API","title":"DynamicPPL.pointwise_loglikelihoods","text":"pointwise_loglikelihoods(model::Model, chain::Chains, keytype = String)\n\nRuns model on each sample in chain returning a OrderedDict{String, Matrix{Float64}} with keys corresponding to symbols of the observations, and values being matrices of shape (num_chains, num_samples).\n\nkeytype specifies what the type of the keys used in the returned OrderedDict are. Currently, only String and VarName are supported.\n\nNotes\n\nSay y is a Vector of n i.i.d. Normal(μ, σ) variables, with μ and σ both being <:Real. Then the observe (i.e. when the left-hand side is an observation) statements can be implemented in three ways:\n\nusing a for loop:\n\nfor i in eachindex(y)\n    y[i] ~ Normal(μ, σ)\nend\n\nusing .~:\n\ny .~ Normal(μ, σ)\n\nusing MvNormal:\n\ny ~ MvNormal(fill(μ, n), σ^2 * I)\n\nIn (1) and (2), y will be treated as a collection of n i.i.d. 1-dimensional variables, while in (3) y will be treated as a single n-dimensional observation.\n\nThis is important to keep in mind, in particular if the computation is used for downstream computations.\n\nExamples\n\nFrom chain\n\njulia> using DynamicPPL, Turing\n\njulia> @model function demo(xs, y)\n           s ~ InverseGamma(2, 3)\n           m ~ Normal(0, √s)\n           for i in eachindex(xs)\n               xs[i] ~ Normal(m, √s)\n           end\n\n           y ~ Normal(m, √s)\n       end\ndemo (generic function with 1 method)\n\njulia> model = demo(randn(3), randn());\n\njulia> chain = sample(model, MH(), 10);\n\njulia> pointwise_loglikelihoods(model, chain)\nOrderedDict{String,Array{Float64,2}} with 4 entries:\n  \"xs[1]\" => [-1.42932; -2.68123; … ; -1.66333; -1.66333]\n  \"xs[2]\" => [-1.6724; -0.861339; … ; -1.62359; -1.62359]\n  \"xs[3]\" => [-1.42862; -2.67573; … ; -1.66251; -1.66251]\n  \"y\"     => [-1.51265; -0.914129; … ; -1.5499; -1.5499]\n\njulia> pointwise_loglikelihoods(model, chain, String)\nOrderedDict{String,Array{Float64,2}} with 4 entries:\n  \"xs[1]\" => [-1.42932; -2.68123; … ; -1.66333; -1.66333]\n  \"xs[2]\" => [-1.6724; -0.861339; … ; -1.62359; -1.62359]\n  \"xs[3]\" => [-1.42862; -2.67573; … ; -1.66251; -1.66251]\n  \"y\"     => [-1.51265; -0.914129; … ; -1.5499; -1.5499]\n\njulia> pointwise_loglikelihoods(model, chain, VarName)\nOrderedDict{VarName,Array{Float64,2}} with 4 entries:\n  xs[1] => [-1.42932; -2.68123; … ; -1.66333; -1.66333]\n  xs[2] => [-1.6724; -0.861339; … ; -1.62359; -1.62359]\n  xs[3] => [-1.42862; -2.67573; … ; -1.66251; -1.66251]\n  y     => [-1.51265; -0.914129; … ; -1.5499; -1.5499]\n\nBroadcasting\n\nNote that x .~ Dist() will treat x as a collection of independent observations rather than as a single observation.\n\njulia> @model function demo(x)\n           x .~ Normal()\n       end;\n\njulia> m = demo([1.0, ]);\n\njulia> ℓ = pointwise_loglikelihoods(m, VarInfo(m)); first(ℓ[@varname(x[1])])\n-1.4189385332046727\n\njulia> m = demo([1.0; 1.0]);\n\njulia> ℓ = pointwise_loglikelihoods(m, VarInfo(m)); first.((ℓ[@varname(x[1])], ℓ[@varname(x[2])]))\n(-1.4189385332046727, -1.4189385332046727)\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API","title":"API","text":"For converting a chain into a format that can more easily be fed into a Model again, for example using condition, you can use value_iterator_from_chain.","category":"page"},{"location":"api/","page":"API","title":"API","text":"value_iterator_from_chain\n","category":"page"},{"location":"api/#DynamicPPL.value_iterator_from_chain","page":"API","title":"DynamicPPL.value_iterator_from_chain","text":"value_iterator_from_chain(model::Model, chain)\nvalue_iterator_from_chain(varinfo::AbstractVarInfo, chain)\n\nReturn an iterator over the values in chain for each variable in model/varinfo.\n\nExample\n\njulia> using MCMCChains, DynamicPPL, Distributions, StableRNGs\n\njulia> rng = StableRNG(42);\n\njulia> @model function demo_model(x)\n           s ~ InverseGamma(2, 3)\n           m ~ Normal(0, sqrt(s))\n           for i in eachindex(x)\n               x[i] ~ Normal(m, sqrt(s))\n           end\n\n           return s, m\n       end\ndemo_model (generic function with 2 methods)\n\njulia> model = demo_model([1.0, 2.0]);\n\njulia> chain = Chains(rand(rng, 10, 2, 3), [:s, :m]);\n\njulia> iter = value_iterator_from_chain(model, chain);\n\njulia> first(iter)\nOrderedDict{VarName, Any} with 2 entries:\n  s => 0.580515\n  m => 0.739328\n\njulia> collect(iter)\n10×3 Matrix{OrderedDict{VarName, Any}}:\n OrderedDict(s=>0.580515, m=>0.739328)  …  OrderedDict(s=>0.186047, m=>0.402423)\n OrderedDict(s=>0.191241, m=>0.627342)     OrderedDict(s=>0.776277, m=>0.166342)\n OrderedDict(s=>0.971133, m=>0.637584)     OrderedDict(s=>0.651655, m=>0.712044)\n OrderedDict(s=>0.74345, m=>0.110359)      OrderedDict(s=>0.469214, m=>0.104502)\n OrderedDict(s=>0.170969, m=>0.598514)     OrderedDict(s=>0.853546, m=>0.185399)\n OrderedDict(s=>0.704776, m=>0.322111)  …  OrderedDict(s=>0.638301, m=>0.853802)\n OrderedDict(s=>0.441044, m=>0.162285)     OrderedDict(s=>0.852959, m=>0.0956922)\n OrderedDict(s=>0.803972, m=>0.643369)     OrderedDict(s=>0.245049, m=>0.871985)\n OrderedDict(s=>0.772384, m=>0.646323)     OrderedDict(s=>0.906603, m=>0.385502)\n OrderedDict(s=>0.70882, m=>0.253105)      OrderedDict(s=>0.413222, m=>0.953288)\n\njulia> # This can be used to `condition` a `Model`.\n       conditioned_model = model | first(iter);\n\njulia> conditioned_model()  # <= results in same values as the `first(iter)` above\n(0.5805148626851955, 0.7393275279160691)\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API","title":"API","text":"Sometimes it can be useful to extract the priors of a model. This is the possible using extract_priors.","category":"page"},{"location":"api/","page":"API","title":"API","text":"extract_priors","category":"page"},{"location":"api/#DynamicPPL.extract_priors","page":"API","title":"DynamicPPL.extract_priors","text":"extract_priors([rng::Random.AbstractRNG, ]model::Model)\n\nExtract the priors from a model.\n\nThis is done by sampling from the model and recording the distributions that are used to generate the samples.\n\nwarning: Warning\nBecause the extraction is done by execution of the model, there are several caveats:If one variable, say, y ~ Normal(0, x), where x ~ Normal() is also a random variable, then the extracted prior will have different parameters in every extraction!\nIf the model does not have static support, say, n ~ Categorical(1:10); x ~ MvNormmal(zeros(n), I), then the extracted priors themselves will be different between extractions, not just their parameters.Both of these caveats are demonstrated below.\n\nExamples\n\nChanging parameters\n\njulia> using Distributions, StableRNGs\n\njulia> rng = StableRNG(42);\n\njulia> @model function model_dynamic_parameters()\n           x ~ Normal(0, 1)\n           y ~ Normal(x, 1)\n       end;\n\njulia> model = model_dynamic_parameters();\n\njulia> extract_priors(rng, model)[@varname(y)]\nNormal{Float64}(μ=-0.6702516921145671, σ=1.0)\n\njulia> extract_priors(rng, model)[@varname(y)]\nNormal{Float64}(μ=1.3736306979834252, σ=1.0)\n\nChanging support\n\njulia> using LinearAlgebra, Distributions, StableRNGs\n\njulia> rng = StableRNG(42);\n\njulia> @model function model_dynamic_support()\n           n ~ Categorical(ones(10) ./ 10)\n           x ~ MvNormal(zeros(n), I)\n       end;\n\njulia> model = model_dynamic_support();\n\njulia> length(extract_priors(rng, model)[@varname(x)])\n6\n\njulia> length(extract_priors(rng, model)[@varname(x)])\n9\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API","title":"API","text":"NamedDist","category":"page"},{"location":"api/#DynamicPPL.NamedDist","page":"API","title":"DynamicPPL.NamedDist","text":"A named distribution that carries the name of the random variable with it.\n\n\n\n\n\n","category":"type"},{"location":"api/#Testing-Utilities","page":"API","title":"Testing Utilities","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"DynamicPPL provides several demo models and helpers for testing samplers in the DynamicPPL.TestUtils submodule.","category":"page"},{"location":"api/","page":"API","title":"API","text":"DynamicPPL.TestUtils.test_sampler\nDynamicPPL.TestUtils.test_sampler_on_demo_models\nDynamicPPL.TestUtils.test_sampler_continuous\nDynamicPPL.TestUtils.marginal_mean_of_samples","category":"page"},{"location":"api/#DynamicPPL.TestUtils.test_sampler","page":"API","title":"DynamicPPL.TestUtils.test_sampler","text":"test_sampler(models, sampler, args...; kwargs...)\n\nTest that sampler produces correct marginal posterior means on each model in models.\n\nIn short, this method iterates through models, calls AbstractMCMC.sample on the model and sampler to produce a chain, and then checks marginal_mean_of_samples(chain, vn) for every (leaf) varname vn against the corresponding value returned by posterior_mean for each model.\n\nTo change how comparison is done for a particular chain type, one can overload marginal_mean_of_samples for the corresponding type.\n\nArguments\n\nmodels: A collection of instaces of DynamicPPL.Model to test on.\nsampler: The AbstractMCMC.AbstractSampler to test.\nargs...: Arguments forwarded to sample.\n\nKeyword arguments\n\nvarnames_filter: A filter to apply to varnames(model), allowing comparison for only   a subset of the varnames.\natol=1e-1: Absolute tolerance used in @test.\nrtol=1e-3: Relative tolerance used in @test.\nkwargs...: Keyword arguments forwarded to sample.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.TestUtils.test_sampler_on_demo_models","page":"API","title":"DynamicPPL.TestUtils.test_sampler_on_demo_models","text":"test_sampler_on_demo_models(meanfunction, sampler, args...; kwargs...)\n\nTest sampler on every model in DEMO_MODELS.\n\nThis is just a proxy for test_sampler(meanfunction, DEMO_MODELS, sampler, args...; kwargs...).\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.TestUtils.test_sampler_continuous","page":"API","title":"DynamicPPL.TestUtils.test_sampler_continuous","text":"test_sampler_continuous(sampler, args...; kwargs...)\n\nTest that sampler produces the correct marginal posterior means on all models in demo_models.\n\nAs of right now, this is just an alias for test_sampler_on_demo_models.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.TestUtils.marginal_mean_of_samples","page":"API","title":"DynamicPPL.TestUtils.marginal_mean_of_samples","text":"marginal_mean_of_samples(chain, varname)\n\nReturn the mean of variable represented by varname in chain.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API","title":"API","text":"DynamicPPL.TestUtils.DEMO_MODELS","category":"page"},{"location":"api/#DynamicPPL.TestUtils.DEMO_MODELS","page":"API","title":"DynamicPPL.TestUtils.DEMO_MODELS","text":"A collection of models corresponding to the posterior distribution defined by the generative process\n\ns ~ InverseGamma(2, 3)\nm ~ Normal(0, √s)\n1.5 ~ Normal(m, √s)\n2.0 ~ Normal(m, √s)\n\nor by\n\ns[1] ~ InverseGamma(2, 3)\ns[2] ~ InverseGamma(2, 3)\nm[1] ~ Normal(0, √s)\nm[2] ~ Normal(0, √s)\n1.5 ~ Normal(m[1], √s[1])\n2.0 ~ Normal(m[2], √s[2])\n\nThese are examples of a Normal-InverseGamma conjugate prior with Normal likelihood, for which the posterior is known in closed form.\n\nIn particular, for the univariate model (the former one):\n\nmean(s) == 49 / 24\nmean(m) == 7 / 6\n\nAnd for the multivariate one (the latter one):\n\nmean(s[1]) == 19 / 8\nmean(m[1]) == 3 / 4\nmean(s[2]) == 8 / 3\nmean(m[2]) == 1\n\n\n\n\n\n","category":"constant"},{"location":"api/","page":"API","title":"API","text":"For every demo model, one can define the true log prior, log likelihood, and log joint probabilities.","category":"page"},{"location":"api/","page":"API","title":"API","text":"DynamicPPL.TestUtils.logprior_true\nDynamicPPL.TestUtils.loglikelihood_true\nDynamicPPL.TestUtils.logjoint_true","category":"page"},{"location":"api/#DynamicPPL.TestUtils.logprior_true","page":"API","title":"DynamicPPL.TestUtils.logprior_true","text":"logprior_true(model, args...)\n\nReturn the logprior of model for args.\n\nThis should generally be implemented by hand for every specific model.\n\nSee also: logjoint_true, loglikelihood_true.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.TestUtils.loglikelihood_true","page":"API","title":"DynamicPPL.TestUtils.loglikelihood_true","text":"loglikelihood_true(model, args...)\n\nReturn the loglikelihood of model for args.\n\nThis should generally be implemented by hand for every specific model.\n\nSee also: logjoint_true, logprior_true.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.TestUtils.logjoint_true","page":"API","title":"DynamicPPL.TestUtils.logjoint_true","text":"logjoint_true(model, args...)\n\nReturn the logjoint of model for args.\n\nDefaults to logprior_true(model, args...) + loglikelihood_true(model, args..).\n\nThis should generally be implemented by hand for every specific model so that the returned value can be used as a ground-truth for testing things like:\n\nValidity of evaluation of model using a particular implementation of AbstractVarInfo.\nValidity of a sampler when combined with DynamicPPL by running the sampler twice: once targeting ground-truth functions, e.g. logjoint_true, and once targeting model.\n\nAnd more.\n\nSee also: logprior_true, loglikelihood_true.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API","title":"API","text":"And in the case where the model includes constrained variables, it can also be useful to define","category":"page"},{"location":"api/","page":"API","title":"API","text":"DynamicPPL.TestUtils.logprior_true_with_logabsdet_jacobian\nDynamicPPL.TestUtils.logjoint_true_with_logabsdet_jacobian","category":"page"},{"location":"api/#DynamicPPL.TestUtils.logprior_true_with_logabsdet_jacobian","page":"API","title":"DynamicPPL.TestUtils.logprior_true_with_logabsdet_jacobian","text":"logprior_true_with_logabsdet_jacobian(model::Model, args...)\n\nReturn a tuple (args_unconstrained, logprior_unconstrained) of model for args....\n\nUnlike logprior_true, the returned logprior computation includes the log-absdet-jacobian adjustment, thus computing logprior for the unconstrained variables.\n\nNote that args are assumed be in the support of model, while args_unconstrained will be unconstrained.\n\nSee also: logprior_true.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.TestUtils.logjoint_true_with_logabsdet_jacobian","page":"API","title":"DynamicPPL.TestUtils.logjoint_true_with_logabsdet_jacobian","text":"logjoint_true_with_logabsdet_jacobian(model::Model, args...)\n\nReturn a tuple (args_unconstrained, logjoint) of model for args.\n\nUnlike logjoint_true, the returned logjoint computation includes the log-absdet-jacobian adjustment, thus computing logjoint for the unconstrained variables.\n\nNote that args are assumed be in the support of model, while args_unconstrained will be unconstrained.\n\nThis should generally not be implemented directly, instead one should implement logprior_true_with_logabsdet_jacobian for a given model.\n\nSee also: logjoint_true, logprior_true_with_logabsdet_jacobian.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API","title":"API","text":"Finally, the following methods can also be of use:","category":"page"},{"location":"api/","page":"API","title":"API","text":"DynamicPPL.TestUtils.varnames\nDynamicPPL.TestUtils.posterior_mean\nDynamicPPL.TestUtils.setup_varinfos\nDynamicPPL.TestUtils.update_values!!\nDynamicPPL.TestUtils.test_values","category":"page"},{"location":"api/#DynamicPPL.TestUtils.varnames","page":"API","title":"DynamicPPL.TestUtils.varnames","text":"varnames(model::Model)\n\nReturn a collection of VarName as they are expected to appear in the model.\n\nEven though it is recommended to implement this by hand for a particular Model, a default implementation using SimpleVarInfo{<:Dict} is provided.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.TestUtils.posterior_mean","page":"API","title":"DynamicPPL.TestUtils.posterior_mean","text":"posterior_mean(model::Model)\n\nReturn a NamedTuple compatible with varnames(model) where the values represent the posterior mean under model.\n\n\"Compatible\" means that a varname from varnames(model) can be used to extract the corresponding value using get, e.g. get(posterior_mean(model), varname).\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.TestUtils.setup_varinfos","page":"API","title":"DynamicPPL.TestUtils.setup_varinfos","text":"setup_varinfos(model::Model, example_values::NamedTuple, varnames)\n\nReturn a tuple of instances for different implementations of AbstractVarInfo with each vi, supposedly, satisfying vi[vn] == get(example_values, vn) for vn in varnames.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.TestUtils.update_values!!","page":"API","title":"DynamicPPL.TestUtils.update_values!!","text":"update_values!!(vi::AbstractVarInfo, vals::NamedTuple, vns)\n\nReturn instance similar to vi but with vns set to values from vals.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.TestUtils.test_values","page":"API","title":"DynamicPPL.TestUtils.test_values","text":"test_values(vi::AbstractVarInfo, vals::NamedTuple, vns)\n\nTest that vi[vn] corresponds to the correct value in vals for every vn in vns.\n\n\n\n\n\n","category":"function"},{"location":"api/#Advanced","page":"API","title":"Advanced","text":"","category":"section"},{"location":"api/#Variable-names","page":"API","title":"Variable names","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Names and possibly nested indices of variables are described with AbstractPPL.VarName. They can be defined with AbstractPPL.@varname. Please see the documentation of AbstractPPL.jl for further information.","category":"page"},{"location":"api/#Data-Structures-of-Variables","page":"API","title":"Data Structures of Variables","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"DynamicPPL provides different data structures for samples from the model and their log density. All of them are subtypes of AbstractVarInfo.","category":"page"},{"location":"api/","page":"API","title":"API","text":"AbstractVarInfo","category":"page"},{"location":"api/#DynamicPPL.AbstractVarInfo","page":"API","title":"DynamicPPL.AbstractVarInfo","text":"AbstractVarInfo\n\nAbstract supertype for data structures that capture random variables when executing a probabilistic model and accumulate log densities such as the log likelihood or the log joint probability of the model.\n\nSee also: VarInfo, SimpleVarInfo.\n\n\n\n\n\n","category":"type"},{"location":"api/#Common-API","page":"API","title":"Common API","text":"","category":"section"},{"location":"api/#Accumulation-of-log-probabilities","page":"API","title":"Accumulation of log-probabilities","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"getlogp\nsetlogp!!\nacclogp!!\nresetlogp!!","category":"page"},{"location":"api/#DynamicPPL.getlogp","page":"API","title":"DynamicPPL.getlogp","text":"getlogp(vi::AbstractVarInfo)\n\nReturn the log of the joint probability of the observed data and parameters sampled in vi.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.setlogp!!","page":"API","title":"DynamicPPL.setlogp!!","text":"setlogp!!(vi::AbstractVarInfo, logp)\n\nSet the log of the joint probability of the observed data and parameters sampled in vi to logp, mutating if it makes sense.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.acclogp!!","page":"API","title":"DynamicPPL.acclogp!!","text":"acclogp!!(vi::AbstractVarInfo, logp)\n\nAdd logp to the value of the log of the joint probability of the observed data and parameters sampled in vi, mutating if it makes sense.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.resetlogp!!","page":"API","title":"DynamicPPL.resetlogp!!","text":"resetlogp!!(vi::AbstractVarInfo)\n\nReset the value of the log of the joint probability of the observed data and parameters sampled in vi to 0, mutating if it makes sense.\n\n\n\n\n\n","category":"function"},{"location":"api/#Variables-and-their-realizations","page":"API","title":"Variables and their realizations","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"keys\ngetindex\nDynamicPPL.getindex_raw\npush!!\nempty!!\nisempty","category":"page"},{"location":"api/#Base.keys","page":"API","title":"Base.keys","text":"keys(vi::AbstractVarInfo)\n\nReturn an iterator over all vns in vi.\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.getindex","page":"API","title":"Base.getindex","text":"getindex(vi::AbstractVarInfo, vn::VarName[, dist::Distribution])\ngetindex(vi::AbstractVarInfo, vns::Vector{<:VarName}[, dist::Distribution])\n\nReturn the current value(s) of vn (vns) in vi in the support of its (their) distribution(s).\n\nIf dist is specified, the value(s) will be reshaped accordingly.\n\nSee also: getindex_raw(vi::AbstractVarInfo, vn::VarName, dist::Distribution)\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.getindex_raw","page":"API","title":"DynamicPPL.getindex_raw","text":"getindex_raw(vi::AbstractVarInfo, vn::VarName[, dist::Distribution])\ngetindex_raw(vi::AbstractVarInfo, vns::Vector{<:VarName}[, dist::Distribution])\n\nReturn the current value(s) of vn (vns) in vi.\n\nIf dist is specified, the value(s) will be reshaped accordingly.\n\nSee also: getindex(vi::AbstractVarInfo, vn::VarName, dist::Distribution)\n\nnote: Note\nThe difference between getindex(vi, vn, dist) and getindex_raw is that  getindex will also transform the value(s) to the support of the distribution(s).  This is not the case for getindex_raw.\n\n\n\n\n\n","category":"function"},{"location":"api/#BangBang.push!!","page":"API","title":"BangBang.push!!","text":"push!!(vi::AbstractVarInfo, vn::VarName, r, dist::Distribution)\n\nPush a new random variable vn with a sampled value r from a distribution dist to the VarInfo vi, mutating if it makes sense.\n\n\n\n\n\npush!!(vi::AbstractVarInfo, vn::VarName, r, dist::Distribution, spl::AbstractSampler)\n\nPush a new random variable vn with a sampled value r sampled with a sampler spl from a distribution dist to VarInfo vi, if it makes sense.\n\nThe sampler is passed here to invalidate its cache where defined.\n\nwarning: Warning\nThis method is considered legacy, and is likely to be deprecated in the future.\n\n\n\n\n\npush!!(vi::AbstractVarInfo, vn::VarName, r, dist::Distribution, gid::Selector)\n\nPush a new random variable vn with a sampled value r sampled with a sampler of selector gid from a distribution dist to VarInfo vi.\n\nwarning: Warning\nThis method is considered legacy, and is likely to be deprecated in the future.\n\n\n\n\n\n","category":"function"},{"location":"api/#BangBang.empty!!","page":"API","title":"BangBang.empty!!","text":"empty!!(vi::AbstractVarInfo)\n\nEmpty the fields of vi.metadata and reset vi.logp[] and vi.num_produce[] to zeros.\n\nThis is useful when using a sampling algorithm that assumes an empty vi, e.g. SMC.\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.isempty","page":"API","title":"Base.isempty","text":"isempty(vi::AbstractVarInfo)\n\nReturn true if vi is empty and false otherwise.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API","title":"API","text":"values_as","category":"page"},{"location":"api/#DynamicPPL.values_as","page":"API","title":"DynamicPPL.values_as","text":"values_as(varinfo[, Type])\n\nReturn the values/realizations in varinfo as Type, if implemented.\n\nIf no Type is provided, return values as stored in varinfo.\n\nExamples\n\nSimpleVarInfo with NamedTuple:\n\njulia> data = (x = 1.0, m = [2.0]);\n\njulia> values_as(SimpleVarInfo(data))\n(x = 1.0, m = [2.0])\n\njulia> values_as(SimpleVarInfo(data), NamedTuple)\n(x = 1.0, m = [2.0])\n\njulia> values_as(SimpleVarInfo(data), OrderedDict)\nOrderedDict{VarName{sym, Setfield.IdentityLens} where sym, Any} with 2 entries:\n  x => 1.0\n  m => [2.0]\n\njulia> values_as(SimpleVarInfo(data), Vector)\n2-element Vector{Float64}:\n 1.0\n 2.0\n\nSimpleVarInfo with OrderedDict:\n\njulia> data = OrderedDict{Any,Any}(@varname(x) => 1.0, @varname(m) => [2.0]);\n\njulia> values_as(SimpleVarInfo(data))\nOrderedDict{Any, Any} with 2 entries:\n  x => 1.0\n  m => [2.0]\n\njulia> values_as(SimpleVarInfo(data), NamedTuple)\n(x = 1.0, m = [2.0])\n\njulia> values_as(SimpleVarInfo(data), OrderedDict)\nOrderedDict{Any, Any} with 2 entries:\n  x => 1.0\n  m => [2.0]\n\njulia> values_as(SimpleVarInfo(data), Vector)\n2-element Vector{Float64}:\n 1.0\n 2.0\n\nTypedVarInfo:\n\njulia> # Just use an example model to construct the `VarInfo` because we're lazy.\n       vi = VarInfo(DynamicPPL.TestUtils.demo_assume_dot_observe());\n\njulia> vi[@varname(s)] = 1.0; vi[@varname(m)] = 2.0;\n\njulia> # For the sake of brevity, let's just check the type.\n       md = values_as(vi); md.s isa DynamicPPL.Metadata\ntrue\n\njulia> values_as(vi, NamedTuple)\n(s = 1.0, m = 2.0)\n\njulia> values_as(vi, OrderedDict)\nOrderedDict{VarName{sym, Setfield.IdentityLens} where sym, Float64} with 2 entries:\n  s => 1.0\n  m => 2.0\n\njulia> values_as(vi, Vector)\n2-element Vector{Float64}:\n 1.0\n 2.0\n\nUntypedVarInfo:\n\njulia> # Just use an example model to construct the `VarInfo` because we're lazy.\n       vi = VarInfo(); DynamicPPL.TestUtils.demo_assume_dot_observe()(vi);\n\njulia> vi[@varname(s)] = 1.0; vi[@varname(m)] = 2.0;\n\njulia> # For the sake of brevity, let's just check the type.\n       values_as(vi) isa DynamicPPL.Metadata\ntrue\n\njulia> values_as(vi, NamedTuple)\n(s = 1.0, m = 2.0)\n\njulia> values_as(vi, OrderedDict)\nOrderedDict{VarName{sym, Setfield.IdentityLens} where sym, Float64} with 2 entries:\n  s => 1.0\n  m => 2.0\n\njulia> values_as(vi, Vector)\n2-element Vector{Real}:\n 1.0\n 2.0\n\n\n\n\n\n","category":"function"},{"location":"api/#Transformations","page":"API","title":"Transformations","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"DynamicPPL.AbstractTransformation\nDynamicPPL.NoTransformation\nDynamicPPL.DynamicTransformation\nDynamicPPL.StaticTransformation","category":"page"},{"location":"api/#DynamicPPL.AbstractTransformation","page":"API","title":"DynamicPPL.AbstractTransformation","text":"abstract type AbstractTransformation\n\nRepresents a transformation to be used in link!! and invlink!!, amongst others.\n\nA concrete implementation of this should implement the following methods:\n\nlink!!: transforms the AbstractVarInfo to the unconstrained space.\ninvlink!!: transforms the AbstractVarInfo to the constrained space.\n\nAnd potentially:\n\nmaybe_invlink_before_eval!!: hook to decide whether to transform before evaluating the model.\n\nSee also: link!!, invlink!!, maybe_invlink_before_eval!!.\n\n\n\n\n\n","category":"type"},{"location":"api/#DynamicPPL.NoTransformation","page":"API","title":"DynamicPPL.NoTransformation","text":"struct NoTransformation <: DynamicPPL.AbstractTransformation\n\nTransformation which applies the identity function.\n\n\n\n\n\n","category":"type"},{"location":"api/#DynamicPPL.DynamicTransformation","page":"API","title":"DynamicPPL.DynamicTransformation","text":"struct DynamicTransformation <: DynamicPPL.AbstractTransformation\n\nTransformation which transforms the variables on a per-need-basis in the execution of a given Model.\n\nThis is in constrast to StaticTransformation which transforms all variables before the execution of a given Model.\n\nSee also: StaticTransformation.\n\n\n\n\n\n","category":"type"},{"location":"api/#DynamicPPL.StaticTransformation","page":"API","title":"DynamicPPL.StaticTransformation","text":"struct StaticTransformation{F} <: DynamicPPL.AbstractTransformation\n\nTransformation which transforms all variables before the execution of a given Model.\n\nThis is done through the maybe_invlink_before_eval!! method.\n\nSee also: DynamicTransformation, maybe_invlink_before_eval!!.\n\nFields\n\nbijector::Any: The function, assumed to implement the Bijectors interface, to be applied to the variables\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API","title":"API","text":"DynamicPPL.istrans\nDynamicPPL.settrans!!\nDynamicPPL.transformation\nDynamicPPL.link\nDynamicPPL.invlink\nDynamicPPL.link!!\nDynamicPPL.invlink!!\nDynamicPPL.default_transformation\nDynamicPPL.maybe_invlink_before_eval!!\nDynamicPPL.reconstruct","category":"page"},{"location":"api/#DynamicPPL.istrans","page":"API","title":"DynamicPPL.istrans","text":"istrans(vi::AbstractVarInfo[, vns::Union{VarName, AbstractVector{<:Varname}}])\n\nReturn true if vi is working in unconstrained space, and false if vi is assuming realizations to be in support of the corresponding distributions.\n\nIf vns is provided, then only check if this/these varname(s) are transformed.\n\nwarning: Warning\nNot all implementations of AbstractVarInfo support transforming only a subset of the variables.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.settrans!!","page":"API","title":"DynamicPPL.settrans!!","text":"settrans!!(vi::AbstractVarInfo, trans::Bool[, vn::VarName])\n\nReturn vi with istrans(vi, vn) evaluating to true.\n\nIf vn is not specified, then istrans(vi) evaluates to true for all variables.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.transformation","page":"API","title":"DynamicPPL.transformation","text":"transformation(vi::AbstractVarInfo)\n\nReturn the AbstractTransformation related to vi.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.link","page":"API","title":"DynamicPPL.link","text":"link([t::AbstractTransformation, ]vi::AbstractVarInfo, model::Model)\nlink([t::AbstractTransformation, ]vi::AbstractVarInfo, spl::AbstractSampler, model::Model)\n\nTransform the variables in vi to their linked space without mutating vi, using the transformation t. \n\nIf t is not provided, default_transformation(model, vi) will be used.\n\nSee also: default_transformation, invlink.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.invlink","page":"API","title":"DynamicPPL.invlink","text":"invlink([t::AbstractTransformation, ]vi::AbstractVarInfo, model::Model)\ninvlink([t::AbstractTransformation, ]vi::AbstractVarInfo, spl::AbstractSampler, model::Model)\n\nTransform the variables in vi to their constrained space without mutating vi, using the (inverse of) transformation t.\n\nIf t is not provided, default_transformation(model, vi) will be used.\n\nSee also: default_transformation, link.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.link!!","page":"API","title":"DynamicPPL.link!!","text":"link!!([t::AbstractTransformation, ]vi::AbstractVarInfo, model::Model)\nlink!!([t::AbstractTransformation, ]vi::AbstractVarInfo, spl::AbstractSampler, model::Model)\n\nTransform the variables in vi to their linked space, using the transformation t, mutating vi if possible.\n\nIf t is not provided, default_transformation(model, vi) will be used.\n\nSee also: default_transformation, invlink!!.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.invlink!!","page":"API","title":"DynamicPPL.invlink!!","text":"invlink!!([t::AbstractTransformation, ]vi::AbstractVarInfo, model::Model)\ninvlink!!([t::AbstractTransformation, ]vi::AbstractVarInfo, spl::AbstractSampler, model::Model)\n\nTransform the variables in vi to their constrained space, using the (inverse of)  transformation t, mutating vi if possible.\n\nIf t is not provided, default_transformation(model, vi) will be used.\n\nSee also: default_transformation, link!!.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.default_transformation","page":"API","title":"DynamicPPL.default_transformation","text":"default_transformation(model::Model[, vi::AbstractVarInfo])\n\nReturn the AbstractTransformation currently related to model and, potentially, vi.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.maybe_invlink_before_eval!!","page":"API","title":"DynamicPPL.maybe_invlink_before_eval!!","text":"maybe_invlink_before_eval!!([t::Transformation,] vi, context, model)\n\nReturn a possibly invlinked version of vi.\n\nThis will be called prior to model evaluation, allowing one to perform a single invlink!! before evaluation rather than lazyily evaluating the transforms on as-we-need basis as is done with DynamicTransformation.\n\nSee also: StaticTransformation, DynamicTransformation.\n\nExamples\n\njulia> using DynamicPPL, Distributions, Bijectors\n\njulia> @model demo() = x ~ Normal()\ndemo (generic function with 2 methods)\n\njulia> # By subtyping `Transform`, we inherit the `(inv)link!!`.\n       struct MyBijector <: Bijectors.Transform end\n\njulia> # Define some dummy `inverse` which will be used in the `link!!` call.\n       Bijectors.inverse(f::MyBijector) = identity\n\njulia> # We need to define `with_logabsdet_jacobian` for `MyBijector`\n       # (`identity` already has `with_logabsdet_jacobian` defined)\n       function Bijectors.with_logabsdet_jacobian(::MyBijector, x)\n           # Just using a large number of the logabsdet-jacobian term\n           # for demonstration purposes.\n           return (x, 1000)\n       end\n\njulia> # Change the `default_transformation` for our model to be a\n       # `StaticTransformation` using `MyBijector`.\n       function DynamicPPL.default_transformation(::Model{typeof(demo)})\n           return DynamicPPL.StaticTransformation(MyBijector())\n       end\n\njulia> model = demo();\n\njulia> vi = SimpleVarInfo(x=1.0)\nSimpleVarInfo((x = 1.0,), 0.0)\n\njulia> # Uses the `inverse` of `MyBijector`, which we have defined as `identity`\n       vi_linked = link!!(vi, model)\nTransformed SimpleVarInfo((x = 1.0,), 0.0)\n\njulia> # Now performs a single `invlink!!` before model evaluation.\n       logjoint(model, vi_linked)\n-1001.4189385332047\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.reconstruct","page":"API","title":"DynamicPPL.reconstruct","text":"reconstruct([f, ]dist, val)\n\nReconstruct val so that it's compatible with dist.\n\nIf f is also provided, the reconstruct value will be such that f(reconstruct_val) is compatible with dist.\n\n\n\n\n\n","category":"function"},{"location":"api/#Utils","page":"API","title":"Utils","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"DynamicPPL.unflatten\nDynamicPPL.tonamedtuple\nDynamicPPL.varname_leaves\nDynamicPPL.varname_and_value_leaves","category":"page"},{"location":"api/#DynamicPPL.unflatten","page":"API","title":"DynamicPPL.unflatten","text":"unflatten(original, x::AbstractVector)\n\nReturn instance of original constructed from x.\n\n\n\n\n\nunflatten(vi::AbstractVarInfo[, context::AbstractContext], x::AbstractVector)\n\nReturn a new instance of vi with the values of x assigned to the variables.\n\nIf context is provided, x is assumed to be realizations only for variables not filtered out by context.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.tonamedtuple","page":"API","title":"DynamicPPL.tonamedtuple","text":"tonamedtuple(vi::AbstractVarInfo)\n\nConvert a vi into a NamedTuple where each variable symbol maps to the values and  indexing string of the variable.\n\nFor example, a model that had a vector of vector-valued variables x would return\n\n(x = ([1.5, 2.0], [3.0, 1.0], [\"x[1]\", \"x[2]\"]), )\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.varname_leaves","page":"API","title":"DynamicPPL.varname_leaves","text":"varname_leaves(vn::VarName, val)\n\nReturn an iterator over all varnames that are represented by vn on val.\n\nExamples\n\njulia> using DynamicPPL: varname_leaves\n\njulia> foreach(println, varname_leaves(@varname(x), rand(2)))\nx[1]\nx[2]\n\njulia> foreach(println, varname_leaves(@varname(x[1:2]), rand(2)))\nx[1:2][1]\nx[1:2][2]\n\njulia> x = (y = 1, z = [[2.0], [3.0]]);\n\njulia> foreach(println, varname_leaves(@varname(x), x))\nx.y\nx.z[1][1]\nx.z[2][1]\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.varname_and_value_leaves","page":"API","title":"DynamicPPL.varname_and_value_leaves","text":"varname_and_value_leaves(vn::VarName, val)\n\nReturn an iterator over all varname-value pairs that are represented by vn on val.\n\nExamples\n\njulia> using DynamicPPL: varname_and_value_leaves\n\njulia> foreach(println, varname_and_value_leaves(@varname(x), 1:2))\n(x[1], 1)\n(x[2], 2)\n\njulia> foreach(println, varname_and_value_leaves(@varname(x[1:2]), 1:2))\n(x[1:2][1], 1)\n(x[1:2][2], 2)\n\njulia> x = (y = 1, z = [[2.0], [3.0]]);\n\njulia> foreach(println, varname_and_value_leaves(@varname(x), x))\n(x.y, 1)\n(x.z[1][1], 2.0)\n(x.z[2][1], 3.0)\n\nThere are also some special handling for certain types:\n\njulia> using LinearAlgebra\n\njulia> x = reshape(1:4, 2, 2);\n\njulia> # `LowerTriangular`\n       foreach(println, varname_and_value_leaves(@varname(x), LowerTriangular(x)))\n(x[1,1], 1)\n(x[2,1], 2)\n(x[2,2], 4)\n\njulia> # `UpperTriangular`\n       foreach(println, varname_and_value_leaves(@varname(x), UpperTriangular(x)))\n(x[1,1], 1)\n(x[1,2], 3)\n(x[2,2], 4)\n\njulia> # `Cholesky` with lower-triangular\n       foreach(println, varname_and_value_leaves(@varname(x), Cholesky([1.0 0.0; 0.0 1.0], 'L', 0)))\n(x.L[1,1], 1.0)\n(x.L[2,1], 0.0)\n(x.L[2,2], 1.0)\n\njulia> # `Cholesky` with upper-triangular\n       foreach(println, varname_and_value_leaves(@varname(x), Cholesky([1.0 0.0; 0.0 1.0], 'U', 0)))\n(x.U[1,1], 1.0)\n(x.U[1,2], 0.0)\n(x.U[2,2], 1.0)\n\n\n\n\n\n","category":"function"},{"location":"api/#SimpleVarInfo","page":"API","title":"SimpleVarInfo","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"SimpleVarInfo","category":"page"},{"location":"api/#DynamicPPL.SimpleVarInfo","page":"API","title":"DynamicPPL.SimpleVarInfo","text":"struct SimpleVarInfo{NT, T, C<:DynamicPPL.AbstractTransformation} <: AbstractVarInfo\n\nA simple wrapper of the parameters with a logp field for accumulation of the logdensity.\n\nCurrently only implemented for NT<:NamedTuple and NT<:AbstractDict.\n\nFields\n\nvalues: underlying representation of the realization represented\nlogp: holds the accumulated log-probability\ntransformation: represents whether it assumes variables to be transformed\n\nNotes\n\nThe major differences between this and TypedVarInfo are:\n\nSimpleVarInfo does not require linearization.\nSimpleVarInfo can use more efficient bijectors.\nSimpleVarInfo is only type-stable if NT<:NamedTuple and either a) no indexing is used in tilde-statements, or b) the values have been specified with the correct shapes.\n\nExamples\n\nGeneral usage\n\njulia> using StableRNGs\n\njulia> @model function demo()\n           m ~ Normal()\n           x = Vector{Float64}(undef, 2)\n           for i in eachindex(x)\n               x[i] ~ Normal()\n           end\n           return x\n       end\ndemo (generic function with 2 methods)\n\njulia> m = demo();\n\njulia> rng = StableRNG(42);\n\njulia> ### Sampling ###\n       ctx = SamplingContext(rng, SampleFromPrior(), DefaultContext());\n\njulia> # In the `NamedTuple` version we need to provide the place-holder values for\n       # the variables which are using \"containers\", e.g. `Array`.\n       # In this case, this means that we need to specify `x` but not `m`.\n       _, vi = DynamicPPL.evaluate!!(m, SimpleVarInfo((x = ones(2), )), ctx);\n\njulia> # (✓) Vroom, vroom! FAST!!!\n       vi[@varname(x[1])]\n0.4471218424633827\n\njulia> # We can also access arbitrary varnames pointing to `x`, e.g.\n       vi[@varname(x)]\n2-element Vector{Float64}:\n 0.4471218424633827\n 1.3736306979834252\n\njulia> vi[@varname(x[1:2])]\n2-element Vector{Float64}:\n 0.4471218424633827\n 1.3736306979834252\n\njulia> # (×) If we don't provide the container...\n       _, vi = DynamicPPL.evaluate!!(m, SimpleVarInfo(), ctx); vi\nERROR: type NamedTuple has no field x\n[...]\n\njulia> # If one does not know the varnames, we can use a `OrderedDict` instead.\n       _, vi = DynamicPPL.evaluate!!(m, SimpleVarInfo{Float64}(OrderedDict()), ctx);\n\njulia> # (✓) Sort of fast, but only possible at runtime.\n       vi[@varname(x[1])]\n-1.019202452456547\n\njulia> # In addtion, we can only access varnames as they appear in the model!\n       vi[@varname(x)]\nERROR: KeyError: key x not found\n[...]\n\njulia> vi[@varname(x[1:2])]\nERROR: KeyError: key x[1:2] not found\n[...]\n\nTechnically, it's possible to use any implementation of AbstractDict in place of OrderedDict, but OrderedDict ensures that certain operations, e.g. linearization/flattening of the values in the varinfo, are consistent between evaluations. Hence OrderedDict is the preferred implementation of AbstractDict to use here.\n\nYou can also sample in transformed space:\n\njulia> @model demo_constrained() = x ~ Exponential()\ndemo_constrained (generic function with 2 methods)\n\njulia> m = demo_constrained();\n\njulia> _, vi = DynamicPPL.evaluate!!(m, SimpleVarInfo(), ctx);\n\njulia> vi[@varname(x)] # (✓) 0 ≤ x < ∞\n1.8632965762164932\n\njulia> _, vi = DynamicPPL.evaluate!!(m, DynamicPPL.settrans!!(SimpleVarInfo(), true), ctx);\n\njulia> vi[@varname(x)] # (✓) -∞ < x < ∞\n-0.21080155351918753\n\njulia> xs = [last(DynamicPPL.evaluate!!(m, DynamicPPL.settrans!!(SimpleVarInfo(), true), ctx))[@varname(x)] for i = 1:10];\n\njulia> any(xs .< 0)  # (✓) Positive probability mass on negative numbers!\ntrue\n\njulia> # And with `OrderedDict` of course!\n       _, vi = DynamicPPL.evaluate!!(m, DynamicPPL.settrans!!(SimpleVarInfo(OrderedDict()), true), ctx);\n\njulia> vi[@varname(x)] # (✓) -∞ < x < ∞\n0.6225185067787314\n\njulia> xs = [last(DynamicPPL.evaluate!!(m, DynamicPPL.settrans!!(SimpleVarInfo(), true), ctx))[@varname(x)] for i = 1:10];\n\njulia> any(xs .< 0) # (✓) Positive probability mass on negative numbers!\ntrue\n\nEvaluation in transformed space of course also works:\n\njulia> vi = DynamicPPL.settrans!!(SimpleVarInfo((x = -1.0,)), true)\nTransformed SimpleVarInfo((x = -1.0,), 0.0)\n\njulia> # (✓) Positive probability mass on negative numbers!\n       getlogp(last(DynamicPPL.evaluate!!(m, vi, DynamicPPL.DefaultContext())))\n-1.3678794411714423\n\njulia> # While if we forget to indicate that it's transformed:\n       vi = DynamicPPL.settrans!!(SimpleVarInfo((x = -1.0,)), false)\nSimpleVarInfo((x = -1.0,), 0.0)\n\njulia> # (✓) No probability mass on negative numbers!\n       getlogp(last(DynamicPPL.evaluate!!(m, vi, DynamicPPL.DefaultContext())))\n-Inf\n\nIndexing\n\nUsing NamedTuple as underlying storage.\n\njulia> svi_nt = SimpleVarInfo((m = (a = [1.0], ), ));\n\njulia> svi_nt[@varname(m)]\n(a = [1.0],)\n\njulia> svi_nt[@varname(m.a)]\n1-element Vector{Float64}:\n 1.0\n\njulia> svi_nt[@varname(m.a[1])]\n1.0\n\njulia> svi_nt[@varname(m.a[2])]\nERROR: BoundsError: attempt to access 1-element Vector{Float64} at index [2]\n[...]\n\njulia> svi_nt[@varname(m.b)]\nERROR: type NamedTuple has no field b\n[...]\n\nUsing OrderedDict as underlying storage.\n\njulia> svi_dict = SimpleVarInfo(OrderedDict(@varname(m) => (a = [1.0], )));\n\njulia> svi_dict[@varname(m)]\n(a = [1.0],)\n\njulia> svi_dict[@varname(m.a)]\n1-element Vector{Float64}:\n 1.0\n\njulia> svi_dict[@varname(m.a[1])]\n1.0\n\njulia> svi_dict[@varname(m.a[2])]\nERROR: BoundsError: attempt to access 1-element Vector{Float64} at index [2]\n[...]\n\njulia> svi_dict[@varname(m.b)]\nERROR: type NamedTuple has no field b\n[...]\n\n\n\n\n\n","category":"type"},{"location":"api/#VarInfo","page":"API","title":"VarInfo","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Another data structure is VarInfo.","category":"page"},{"location":"api/","page":"API","title":"API","text":"VarInfo\nTypedVarInfo","category":"page"},{"location":"api/#DynamicPPL.VarInfo","page":"API","title":"DynamicPPL.VarInfo","text":"struct VarInfo{Tmeta, Tlogp} <: AbstractVarInfo\n    metadata::Tmeta\n    logp::Base.RefValue{Tlogp}\n    num_produce::Base.RefValue{Int}\nend\n\nA light wrapper over one or more instances of Metadata. Let vi be an instance of VarInfo. If vi isa VarInfo{<:Metadata}, then only one Metadata instance is used for all the sybmols. VarInfo{<:Metadata} is aliased UntypedVarInfo. If vi isa VarInfo{<:NamedTuple}, then vi.metadata is a NamedTuple that maps each symbol used on the LHS of ~ in the model to its Metadata instance. The latter allows for the type specialization of vi after the first sampling iteration when all the symbols have been observed. VarInfo{<:NamedTuple} is aliased TypedVarInfo.\n\nNote: It is the user's responsibility to ensure that each \"symbol\" is visited at least once whenever the model is called, regardless of any stochastic branching. Each symbol refers to a Julia variable and can be a hierarchical array of many random variables, e.g. x[1] ~ ... and x[2] ~ ... both have the same symbol x.\n\n\n\n\n\n","category":"type"},{"location":"api/#DynamicPPL.TypedVarInfo","page":"API","title":"DynamicPPL.TypedVarInfo","text":"TypedVarInfo(vi::UntypedVarInfo)\n\nThis function finds all the unique syms from the instances of VarName{sym} found in vi.metadata.vns. It then extracts the metadata associated with each symbol from the global vi.metadata field. Finally, a new VarInfo is created with a new metadata as a NamedTuple mapping from symbols to type-stable Metadata instances, one for each symbol.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API","title":"API","text":"One main characteristic of VarInfo is that samples are stored in a linearized form.","category":"page"},{"location":"api/","page":"API","title":"API","text":"link!\ninvlink!","category":"page"},{"location":"api/#DynamicPPL.link!","page":"API","title":"DynamicPPL.link!","text":"link!(vi::VarInfo, spl::Sampler)\n\nTransform the values of the random variables sampled by spl in vi from the support of their distributions to the Euclidean space and set their corresponding \"trans\" flag values to true.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.invlink!","page":"API","title":"DynamicPPL.invlink!","text":"invlink!(vi::VarInfo, spl::AbstractSampler)\n\nTransform the values of the random variables sampled by spl in vi from the Euclidean space back to the support of their distributions and sets their corresponding \"trans\" flag values to false.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API","title":"API","text":"set_flag!\nunset_flag!\nis_flagged","category":"page"},{"location":"api/#DynamicPPL.set_flag!","page":"API","title":"DynamicPPL.set_flag!","text":"set_flag!(vi::VarInfo, vn::VarName, flag::String)\n\nSet vn's value for flag to true in vi.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.unset_flag!","page":"API","title":"DynamicPPL.unset_flag!","text":"unset_flag!(vi::VarInfo, vn::VarName, flag::String)\n\nSet vn's value for flag to false in vi.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.is_flagged","page":"API","title":"DynamicPPL.is_flagged","text":"is_flagged(vi::VarInfo, vn::VarName, flag::String)\n\nCheck whether vn has a true value for flag in vi.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API","title":"API","text":"For Gibbs sampling the following functions were added.","category":"page"},{"location":"api/","page":"API","title":"API","text":"setgid!\nupdategid!","category":"page"},{"location":"api/#DynamicPPL.setgid!","page":"API","title":"DynamicPPL.setgid!","text":"setgid!(vi::VarInfo, gid::Selector, vn::VarName)\n\nAdd gid to the set of sampler selectors associated with vn in vi.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.updategid!","page":"API","title":"DynamicPPL.updategid!","text":"updategid!(vi::VarInfo, vn::VarName, spl::Sampler)\n\nSet vn's gid to Set([spl.selector]), if vn does not have a sampler selector linked and vn's symbol is in the space of spl.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API","title":"API","text":"The following functions were used for sequential Monte Carlo methods.","category":"page"},{"location":"api/","page":"API","title":"API","text":"get_num_produce\nset_num_produce!\nincrement_num_produce!\nreset_num_produce!\nsetorder!\nset_retained_vns_del_by_spl!","category":"page"},{"location":"api/#DynamicPPL.get_num_produce","page":"API","title":"DynamicPPL.get_num_produce","text":"get_num_produce(vi::VarInfo)\n\nReturn the num_produce of vi.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.set_num_produce!","page":"API","title":"DynamicPPL.set_num_produce!","text":"set_num_produce!(vi::VarInfo, n::Int)\n\nSet the num_produce field of vi to n.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.increment_num_produce!","page":"API","title":"DynamicPPL.increment_num_produce!","text":"increment_num_produce!(vi::VarInfo)\n\nAdd 1 to num_produce in vi.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.reset_num_produce!","page":"API","title":"DynamicPPL.reset_num_produce!","text":"reset_num_produce!(vi::VarInfo)\n\nReset the value of num_produce the log of the joint probability of the observed data and parameters sampled in vi to 0.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.setorder!","page":"API","title":"DynamicPPL.setorder!","text":"setorder!(vi::VarInfo, vn::VarName, index::Int)\n\nSet the order of vn in vi to index, where order is the number of observe statements run before samplingvn`.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.set_retained_vns_del_by_spl!","page":"API","title":"DynamicPPL.set_retained_vns_del_by_spl!","text":"set_retained_vns_del_by_spl!(vi::VarInfo, spl::Sampler)\n\nSet the \"del\" flag of variables in vi with order > vi.num_produce[] to true.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API","title":"API","text":"Base.empty!","category":"page"},{"location":"api/#Base.empty!","page":"API","title":"Base.empty!","text":"empty!(meta::Metadata)\n\nEmpty the fields of meta.\n\nThis is useful when using a sampling algorithm that assumes an empty meta, e.g. SMC.\n\n\n\n\n\n","category":"function"},{"location":"api/#Evaluation-Contexts","page":"API","title":"Evaluation Contexts","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Internally, both sampling and evaluation of log densities are performed with AbstractPPL.evaluate!!.","category":"page"},{"location":"api/","page":"API","title":"API","text":"AbstractPPL.evaluate!!","category":"page"},{"location":"api/#AbstractPPL.evaluate!!","page":"API","title":"AbstractPPL.evaluate!!","text":"evaluate!!(model::Model[, rng, varinfo, sampler, context])\n\nSample from the model using the sampler with random number generator rng and the context, and store the sample and log joint probability in varinfo.\n\nReturns both the return-value of the original model, and the resulting varinfo.\n\nThe method resets the log joint probability of varinfo and increases the evaluation number of sampler.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API","title":"API","text":"The behaviour of a model execution can be changed with evaluation contexts that are passed as additional argument to the model function. Contexts are subtypes of AbstractPPL.AbstractContext.","category":"page"},{"location":"api/","page":"API","title":"API","text":"SamplingContext\nDefaultContext\nLikelihoodContext\nPriorContext\nMiniBatchContext\nPrefixContext","category":"page"},{"location":"api/#DynamicPPL.SamplingContext","page":"API","title":"DynamicPPL.SamplingContext","text":"SamplingContext(\n        [rng::Random.AbstractRNG=Random.default_rng()],\n        [sampler::AbstractSampler=SampleFromPrior()],\n        [context::AbstractContext=DefaultContext()],\n)\n\nCreate a context that allows you to sample parameters with the sampler when running the model. The context determines how the returned log density is computed when running the model.\n\nSee also: DefaultContext, LikelihoodContext, PriorContext\n\n\n\n\n\n","category":"type"},{"location":"api/#DynamicPPL.DefaultContext","page":"API","title":"DynamicPPL.DefaultContext","text":"struct DefaultContext <: AbstractContext end\n\nThe DefaultContext is used by default to compute log the joint probability of the data  and parameters when running the model.\n\n\n\n\n\n","category":"type"},{"location":"api/#DynamicPPL.LikelihoodContext","page":"API","title":"DynamicPPL.LikelihoodContext","text":"struct LikelihoodContext{Tvars} <: AbstractContext\n    vars::Tvars\nend\n\nThe LikelihoodContext enables the computation of the log likelihood of the parameters when  running the model. vars can be used to evaluate the log likelihood for specific values  of the model's parameters. If vars is nothing, the parameter values inside the VarInfo will be used by default.\n\n\n\n\n\n","category":"type"},{"location":"api/#DynamicPPL.PriorContext","page":"API","title":"DynamicPPL.PriorContext","text":"struct PriorContext{Tvars} <: AbstractContext\n    vars::Tvars\nend\n\nThe PriorContext enables the computation of the log prior of the parameters vars when  running the model.\n\n\n\n\n\n","category":"type"},{"location":"api/#DynamicPPL.MiniBatchContext","page":"API","title":"DynamicPPL.MiniBatchContext","text":"struct MiniBatchContext{Tctx, T} <: AbstractContext\n    context::Tctx\n    loglike_scalar::T\nend\n\nThe MiniBatchContext enables the computation of  log(prior) + s * log(likelihood of a batch) when running the model, where s is the  loglike_scalar field, typically equal to the number of data points / batch size.  This is useful in batch-based stochastic gradient descent algorithms to be optimizing  log(prior) + log(likelihood of all the data points) in the expectation.\n\n\n\n\n\n","category":"type"},{"location":"api/#DynamicPPL.PrefixContext","page":"API","title":"DynamicPPL.PrefixContext","text":"PrefixContext{Prefix}(context)\n\nCreate a context that allows you to use the wrapped context when running the model and adds the Prefix to all parameters.\n\nThis context is useful in nested models to ensure that the names of the parameters are unique.\n\nSee also: @submodel\n\n\n\n\n\n","category":"type"},{"location":"api/#Samplers","page":"API","title":"Samplers","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"In DynamicPPL two samplers are defined that are used to initialize unobserved random variables: SampleFromPrior which samples from the prior distribution, and SampleFromUniform which samples from a uniform distribution.","category":"page"},{"location":"api/","page":"API","title":"API","text":"SampleFromPrior\nSampleFromUniform","category":"page"},{"location":"api/#DynamicPPL.SampleFromPrior","page":"API","title":"DynamicPPL.SampleFromPrior","text":"SampleFromPrior\n\nSampling algorithm that samples unobserved random variables from their prior distribution.\n\n\n\n\n\n","category":"type"},{"location":"api/#DynamicPPL.SampleFromUniform","page":"API","title":"DynamicPPL.SampleFromUniform","text":"SampleFromUniform\n\nSampling algorithm that samples unobserved random variables from a uniform distribution.\n\nReferences\n\nStan reference manual\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API","title":"API","text":"Additionally, a generic sampler for inference is implemented.","category":"page"},{"location":"api/","page":"API","title":"API","text":"Sampler","category":"page"},{"location":"api/#DynamicPPL.Sampler","page":"API","title":"DynamicPPL.Sampler","text":"Sampler{T}\n\nGeneric sampler type for inference algorithms of type T in DynamicPPL.\n\nSampler should implement the AbstractMCMC interface, and in particular AbstractMCMC.step. A default implementation of the initial sampling step is provided that supports resuming sampling from a previous state and setting initial parameter values. It requires to overload loadstate and initialstep for loading previous states and actually performing the initial sampling step, respectively. Additionally, sometimes one might want to implement initialsampler that specifies how the initial parameter values are sampled if they are not provided. By default, values are sampled from the prior.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API","title":"API","text":"The default implementation of Sampler uses the following unexported functions.","category":"page"},{"location":"api/","page":"API","title":"API","text":"DynamicPPL.initialstep\nDynamicPPL.loadstate\nDynamicPPL.initialsampler","category":"page"},{"location":"api/#DynamicPPL.initialstep","page":"API","title":"DynamicPPL.initialstep","text":"initialstep(rng, model, sampler, varinfo; kwargs...)\n\nPerform the initial sampling step of the sampler for the model.\n\nThe varinfo contains the initial samples, which can be provided by the user or sampled randomly.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.loadstate","page":"API","title":"DynamicPPL.loadstate","text":"loadstate(data)\n\nLoad sampler state from data.\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.initialsampler","page":"API","title":"DynamicPPL.initialsampler","text":"initialsampler(sampler::Sampler)\n\nReturn the sampler that is used for generating the initial parameters when sampling with sampler.\n\nBy default, it returns an instance of SampleFromPrior.\n\n\n\n\n\n","category":"function"},{"location":"api/#model_internal","page":"API","title":"Model-Internal Functions","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"tilde_assume\ndot_tilde_assume","category":"page"},{"location":"api/#DynamicPPL.tilde_assume","page":"API","title":"DynamicPPL.tilde_assume","text":"tilde_assume(context::SamplingContext, right, vn, vi)\n\nHandle assumed variables, e.g., x ~ Normal() (where x does occur in the model inputs), accumulate the log probability, and return the sampled value with a context associated with a sampler.\n\nFalls back to\n\ntilde_assume(context.rng, context.context, context.sampler, right, vn, vi)\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.dot_tilde_assume","page":"API","title":"DynamicPPL.dot_tilde_assume","text":"dot_tilde_assume(context::SamplingContext, right, left, vn, vi)\n\nHandle broadcasted assumed variables, e.g., x .~ MvNormal() (where x does not occur in the model inputs), accumulate the log probability, and return the sampled value for a context associated with a sampler.\n\nFalls back to\n\ndot_tilde_assume(context.rng, context.context, context.sampler, right, left, vn, vi)\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API","title":"API","text":"tilde_observe\ndot_tilde_observe","category":"page"},{"location":"api/#DynamicPPL.tilde_observe","page":"API","title":"DynamicPPL.tilde_observe","text":"tilde_observe(context::SamplingContext, right, left, vi)\n\nHandle observed constants with a context associated with a sampler.\n\nFalls back to tilde_observe(context.context, context.sampler, right, left, vi).\n\n\n\n\n\n","category":"function"},{"location":"api/#DynamicPPL.dot_tilde_observe","page":"API","title":"DynamicPPL.dot_tilde_observe","text":"dot_tilde_observe(context::SamplingContext, right, left, vi)\n\nHandle broadcasted observed constants, e.g., [1.0] .~ MvNormal(), accumulate the log probability, and return the observed value for a context associated with a sampler.\n\nFalls back to dot_tilde_observe(context.context, context.sampler, right, left, vi).\n\n\n\n\n\n","category":"function"},{"location":"#DynamicPPL.jl","page":"Home","title":"DynamicPPL.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A domain-specific language and backend for probabilistic programming languages, used by Turing.jl.","category":"page"},{"location":"tutorials/prob-interface/#The-Probability-Interface","page":"The Probability Interface","title":"The Probability Interface","text":"","category":"section"},{"location":"tutorials/prob-interface/","page":"The Probability Interface","title":"The Probability Interface","text":"The easiest way to manipulate and query DynamicPPL models is via the DynamicPPL probability interface.","category":"page"},{"location":"tutorials/prob-interface/","page":"The Probability Interface","title":"The Probability Interface","text":"Let's use a simple model of normally-distributed data as an example.","category":"page"},{"location":"tutorials/prob-interface/","page":"The Probability Interface","title":"The Probability Interface","text":"using DynamicPPL\nusing Distributions\nusing FillArrays\n\nusing LinearAlgebra\nusing Random\n\n@model function gdemo(n)\n    μ ~ Normal(0, 1)\n    x ~ MvNormal(Fill(μ, n), I)\n    return nothing\nend\nnothing # hide","category":"page"},{"location":"tutorials/prob-interface/","page":"The Probability Interface","title":"The Probability Interface","text":"We generate some data using μ = 0:","category":"page"},{"location":"tutorials/prob-interface/","page":"The Probability Interface","title":"The Probability Interface","text":"Random.seed!(1776)\ndataset = randn(100)\nnothing # hide","category":"page"},{"location":"tutorials/prob-interface/#Conditioning-and-Deconditioning","page":"The Probability Interface","title":"Conditioning and Deconditioning","text":"","category":"section"},{"location":"tutorials/prob-interface/","page":"The Probability Interface","title":"The Probability Interface","text":"Bayesian models can be transformed with two main operations, conditioning and deconditioning (also known as marginalization). Conditioning takes a variable and fixes its value as known. We do this by passing a model and a collection of conditioned variables to | or its alias condition:","category":"page"},{"location":"tutorials/prob-interface/","page":"The Probability Interface","title":"The Probability Interface","text":"model = gdemo(length(dataset)) | (x=dataset, μ=0)\nnothing # hide","category":"page"},{"location":"tutorials/prob-interface/","page":"The Probability Interface","title":"The Probability Interface","text":"This operation can be reversed by applying decondition:","category":"page"},{"location":"tutorials/prob-interface/","page":"The Probability Interface","title":"The Probability Interface","text":"decondition(model)\nnothing # hide","category":"page"},{"location":"tutorials/prob-interface/","page":"The Probability Interface","title":"The Probability Interface","text":"We can also decondition only some of the variables:","category":"page"},{"location":"tutorials/prob-interface/","page":"The Probability Interface","title":"The Probability Interface","text":"decondition(model, :μ)\nnothing # hide","category":"page"},{"location":"tutorials/prob-interface/","page":"The Probability Interface","title":"The Probability Interface","text":"note: Note\nSometimes it is helpful to define convenience functions for conditioning on some variable(s). For instance, in this example we might want to define a version of gdemo that conditions on some observations of x:gdemo(x::AbstractVector{<:Real}) = gdemo(length(x)) | (; x)For illustrative purposes, however, we do not use this function in the examples below.","category":"page"},{"location":"tutorials/prob-interface/#Probabilities-and-Densities","page":"The Probability Interface","title":"Probabilities and Densities","text":"","category":"section"},{"location":"tutorials/prob-interface/","page":"The Probability Interface","title":"The Probability Interface","text":"We often want to calculate the (unnormalized) probability density for an event. This probability might be a prior, a likelihood, or a posterior (joint) density. DynamicPPL provides convenient functions for this. For example, we can calculate the joint probability of a set of samples (here drawn from the prior) with logjoint:","category":"page"},{"location":"tutorials/prob-interface/","page":"The Probability Interface","title":"The Probability Interface","text":"model = gdemo(length(dataset)) | (x=dataset,)\n\nRandom.seed!(124)\nsample = rand(model)\nlogjoint(model, sample)","category":"page"},{"location":"tutorials/prob-interface/","page":"The Probability Interface","title":"The Probability Interface","text":"For models with many variables rand(model) can be prohibitively slow since it returns a NamedTuple of samples from the prior distribution of the unconditioned variables. We recommend working with samples of type DataStructures.OrderedDict in this case:","category":"page"},{"location":"tutorials/prob-interface/","page":"The Probability Interface","title":"The Probability Interface","text":"using DataStructures\n\nRandom.seed!(124)\nsample_dict = rand(OrderedDict, model)\nlogjoint(model, sample_dict)","category":"page"},{"location":"tutorials/prob-interface/","page":"The Probability Interface","title":"The Probability Interface","text":"The prior probability and the likelihood of a set of samples can be calculated with the functions loglikelihood and logjoint, respectively:","category":"page"},{"location":"tutorials/prob-interface/","page":"The Probability Interface","title":"The Probability Interface","text":"logjoint(model, sample) ≈ loglikelihood(model, sample) + logprior(model, sample)","category":"page"},{"location":"tutorials/prob-interface/","page":"The Probability Interface","title":"The Probability Interface","text":"logjoint(model, sample_dict) ≈\nloglikelihood(model, sample_dict) + logprior(model, sample_dict)","category":"page"},{"location":"tutorials/prob-interface/#Example:-Cross-validation","page":"The Probability Interface","title":"Example: Cross-validation","text":"","category":"section"},{"location":"tutorials/prob-interface/","page":"The Probability Interface","title":"The Probability Interface","text":"To give an example of the probability interface in use, we can use it to estimate the performance of our model using cross-validation. In cross-validation, we split the dataset into several equal parts. Then, we choose one of these sets to serve as the validation set. Here, we measure fit using the cross entropy (Bayes loss).[1]","category":"page"},{"location":"tutorials/prob-interface/","page":"The Probability Interface","title":"The Probability Interface","text":"using MLUtils\n\nfunction cross_val(\n    dataset::AbstractVector{<:Real};\n    nfolds::Int=5,\n    nsamples::Int=1_000,\n    rng::Random.AbstractRNG=Random.default_rng(),\n)\n    # Initialize `loss` in a way such that the loop below does not change its type\n    model = gdemo(1) | (x=[first(dataset)],)\n    loss = zero(logjoint(model, rand(rng, model)))\n\n    for (train, validation) in kfolds(dataset, nfolds)\n        # First, we train the model on the training set, i.e., we obtain samples from the posterior.\n        # For normally-distributed data, the posterior can be computed in closed form.\n        # For general models, however, typically samples will be generated using MCMC with Turing.\n        posterior = Normal(mean(train), 1)\n        samples = rand(rng, posterior, nsamples)\n\n        # Evaluation on the validation set.\n        validation_model = gdemo(length(validation)) | (x=validation,)\n        loss += sum(samples) do sample\n            logjoint(validation_model, (μ=sample,))\n        end\n    end\n\n    return loss\nend\n\ncross_val(dataset)","category":"page"},{"location":"tutorials/prob-interface/","page":"The Probability Interface","title":"The Probability Interface","text":"[1]: See ParetoSmooth.jl for a faster and more accurate implementation of cross-validation than the one provided here.","category":"page"}]
}
